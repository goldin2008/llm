{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test writing using a multi-step prompt\n",
    "\n",
    "Complex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT-3 and then feeds that text back into subsequent prompts. This can help in cases where you want GPT-3 to explain its reasoning before answering, or brainstorm a plan before executing it.\n",
    "\n",
    "In this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n",
    "\n",
    "1. Given a Python function, we first prompt GPT-3 to explain what the function is doing.\n",
    "2. Second, we prompt GPT-3 to plan a set of unit tests for the function.\n",
    "    - If the plan is too short, we ask GPT-3 to elaborate with more ideas for unit tests.\n",
    "3. Finally, we prompt GPT-3 to write the unit tests.\n",
    "\n",
    "The code example illustrates a few optional embellishments on the chained, multi-step prompt:\n",
    "\n",
    "- Conditional branching (e.g., only asking for elaboration if the first plan is too short)\n",
    "- Different models for different steps (e.g., `text-davinci-002` for the text planning steps and `code-davinci-002` for the code writing step)\n",
    "- A check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's `ast` module)\n",
    "- Streaming output so that you can start reading the output before it's fully generated (useful for long, multi-step outputs)\n",
    "\n",
    "The full 3-step prompt looks like this (using as an example `pytest` for the unit test framework and `is_palindrome` as the function):\n",
    "\n",
    "    # How to write great unit tests with pytest\n",
    "\n",
    "    In this advanced tutorial for experts, we'll use Python 3.9 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
    "    ```python\n",
    "    def is_palindrome(s):\n",
    "        return s == s[::-1]\n",
    "    ```\n",
    "\n",
    "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
    "    - First,{GENERATED IN STEP 1}\n",
    "        \n",
    "    A good unit test suite should aim to:\n",
    "    - Test the function's behavior for a wide range of possible inputs\n",
    "    - Test edge cases that the author may not have foreseen\n",
    "    - Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
    "    - Be easy to read and understand, with clean code and descriptive names\n",
    "    - Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "    `pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
    "\n",
    "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
    "    -{GENERATED IN STEP 2}\n",
    "\n",
    "    [OPTIONALLY APPENDED]In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
    "    -{GENERATED IN STEP 2B}\n",
    "\n",
    "    Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
    "    ```python\n",
    "    import pytest  # used for our unit tests\n",
    "\n",
    "    def is_palindrome(s):\n",
    "        return s == s[::-1]\n",
    "\n",
    "    #Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
    "    {GENERATED IN STEP 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed to run the code in this notebook\n",
    "import ast  # used for detecting whether generated Python code is valid\n",
    "import openai  # used for calling the OpenAI API\n",
    "\n",
    "# example of a function that uses a multi-step prompt to write unit tests\n",
    "def unit_test_from_function(\n",
    "    function_to_test: str,  # Python function to test, as a string\n",
    "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
    "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
    "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
    "    text_model: str = \"text-davinci-002\",  # model used to generate text plans in steps 1, 2, and 2b\n",
    "    # code_model: str = \"code-davinci-002\",  # if you don't have access to code models, you can use text models here instead\n",
    "    code_model: str = \"code-davinci-edit-001\",  # if you don't have access to code models, you can use text models here instead\n",
    "    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n",
    "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
    "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
    ") -> str:\n",
    "    \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n",
    "\n",
    "    # Step 1: Generate an explanation of the function\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n",
    "    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n",
    "\n",
    "In this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
    "```python\n",
    "{function_to_test}\n",
    "```\n",
    "\n",
    "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
    "- First,\"\"\"\n",
    "    if print_text:\n",
    "        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n",
    "        print(text_color_prefix + prompt_to_explain_the_function, end=\"\")  # end='' prevents a newline from being printed\n",
    "\n",
    "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "    explanation_response = openai.Completion.create(\n",
    "        model=text_model,\n",
    "        prompt=prompt_to_explain_the_function,\n",
    "        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    explanation_completion = \"\"\n",
    "    if print_text:\n",
    "        completion_color_prefix = \"\\033[92m\"  # green\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in explanation_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        explanation_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # Step 2: Generate a plan to write a unit test\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n",
    "    prompt_to_explain_a_plan = f\"\"\"\n",
    "    \n",
    "A good unit test suite should aim to:\n",
    "- Test the function's behavior for a wide range of possible inputs\n",
    "- Test edge cases that the author may not have foreseen\n",
    "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
    "- Be easy to read and understand, with clean code and descriptive names\n",
    "- Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
    "\n",
    "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
    "-\"\"\"\n",
    "    if print_text:\n",
    "        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n",
    "\n",
    "    # append this planning prompt to the results from step 1\n",
    "    prior_text = prompt_to_explain_the_function + explanation_completion\n",
    "    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n",
    "\n",
    "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "    plan_response = openai.Completion.create(\n",
    "        model=text_model,\n",
    "        prompt=full_plan_prompt,\n",
    "        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    plan_completion = \"\"\n",
    "    if print_text:\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in plan_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        plan_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n",
    "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
    "    elaboration_needed = plan_completion.count(\"\\n-\") +1 < approx_min_cases_to_cover  # adds 1 because the first bullet is not counted\n",
    "    if elaboration_needed:\n",
    "        prompt_to_elaborate_on_the_plan = f\"\"\"\n",
    "\n",
    "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
    "-\"\"\"\n",
    "        if print_text:\n",
    "            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n",
    "\n",
    "        # append this elaboration prompt to the results from step 2\n",
    "        prior_text = full_plan_prompt + plan_completion\n",
    "        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n",
    "\n",
    "        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "        elaboration_response = openai.Completion.create(\n",
    "            model=text_model,\n",
    "            prompt=full_elaboration_prompt,\n",
    "            stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        elaboration_completion = \"\"\n",
    "        if print_text:\n",
    "            print(completion_color_prefix, end=\"\")\n",
    "        for event in elaboration_response:\n",
    "            event_text = event[\"choices\"][0][\"text\"]\n",
    "            elaboration_completion += event_text\n",
    "            if print_text:\n",
    "                print(event_text, end=\"\")\n",
    "\n",
    "    # Step 3: Generate the unit test\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n",
    "    starter_comment = \"\"\n",
    "    if unit_test_package == \"pytest\":\n",
    "        starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
    "    prompt_to_generate_the_unit_test = f\"\"\"\n",
    "\n",
    "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
    "```python\n",
    "import {unit_test_package}  # used for our unit tests\n",
    "\n",
    "{function_to_test}\n",
    "\n",
    "#{starter_comment}\"\"\"\n",
    "    if print_text:\n",
    "        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n",
    "\n",
    "    # append this unit test prompt to the results from step 3\n",
    "    if elaboration_needed:\n",
    "        prior_text = full_elaboration_prompt + elaboration_completion\n",
    "    else:\n",
    "        prior_text = full_plan_prompt + plan_completion\n",
    "    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n",
    "\n",
    "    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n",
    "    unit_test_response = openai.Completion.create(\n",
    "        model=code_model,\n",
    "        prompt=full_unit_test_prompt,\n",
    "        stop=\"```\",\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True\n",
    "    )\n",
    "    unit_test_completion = \"\"\n",
    "    if print_text:\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in unit_test_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        unit_test_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # check the output for errors\n",
    "    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n",
    "    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n",
    "    try:\n",
    "        ast.parse(code_output)\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax error in generated code: {e}\")\n",
    "        if reruns_if_fail > 0:\n",
    "            print(\"Rerunning...\")\n",
    "            return unit_test_from_function(\n",
    "                function_to_test=function_to_test,\n",
    "                unit_test_package=unit_test_package,\n",
    "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
    "                print_text=print_text,\n",
    "                text_model=text_model,\n",
    "                code_model=code_model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                reruns_if_fail=reruns_if_fail-1,  # decrement rerun counter when calling again\n",
    "            )\n",
    "\n",
    "    # return the unit test as a string\n",
    "    return unit_test_completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m# How to write great unit tests with pytest\n",
      "\n",
      "In this advanced tutorial for experts, we'll use Python 3.9 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
      "```python\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "```\n",
      "\n",
      "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "- First,\u001b[92m we have a function with the name `is_palindrome`.\n",
      "- Second, we have a function parameter named `s`.\n",
      "- Third, we have a return statement that checks if `s` is equal to `s` reversed.\n",
      "- Finally, we have a function call to `is_palindrome` with the string `\"racecar\"` as the `s` parameter.\u001b[30m\n",
      "    \n",
      "A good unit test suite should aim to:\n",
      "- Test the function's behavior for a wide range of possible inputs\n",
      "- Test edge cases that the author may not have foreseen\n",
      "- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
      "- Be easy to read and understand, with clean code and descriptive names\n",
      "- Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "`pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "-\u001b[92m The function should return `True` for strings that are palindromes\n",
      "    - `\"racecar\"`\n",
      "    - `\"anna\"`\n",
      "    - `\"civic\"`\n",
      "- The function should return `False` for strings that are not palindromes\n",
      "    - `\"python\"`\n",
      "    - `\"unittest\"`\n",
      "    - `\"abcdef\"`\n",
      "- The function should handle empty strings\n",
      "    - `\"\"`\n",
      "- The function should handle strings with spaces\n",
      "    - `\"a man a plan a canal panama\"`\n",
      "- The function should handle strings with uppercase letters\n",
      "    - `\"Racecar\"`\n",
      "- The function should handle strings with punctuation\n",
      "    - `\"A man, a plan, a canal, Panama!\"`\n",
      "- The function should handle strings with non-ASCII characters\n",
      "    - `\"Ābādābā\"`\n",
      "- The function should handle strings with mixed case, spaces, and punctuation\n",
      "    - `\"A man, a plan, a canal, Panama!\"`\u001b[30m\n",
      "\n",
      "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
      "```python\n",
      "import pytest  # used for our unit tests\n",
      "\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "\n",
      "#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model is not supported in the v1/completions endpoint.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_5863/3566448537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     return s == s[::-1]\"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0munit_test_from_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_5863/2733184488.py\u001b[0m in \u001b[0;36munit_test_from_function\u001b[0;34m(function_to_test, unit_test_package, approx_min_cases_to_cover, print_text, text_model, code_model, max_tokens, temperature, reruns_if_fail)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     unit_test_response = openai.Completion.create(\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_unit_test_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model is not supported in the v1/completions endpoint."
     ]
    }
   ],
   "source": [
    "example_function = \"\"\"def is_palindrome(s):\n",
    "    return s == s[::-1]\"\"\"\n",
    "\n",
    "unit_test_from_function(example_function, print_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
