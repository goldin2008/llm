{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T06:08:40.769809Z",
     "iopub.status.busy": "2024-04-05T06:08:40.769352Z",
     "iopub.status.idle": "2024-04-05T06:08:40.781397Z",
     "shell.execute_reply": "2024-04-05T06:08:40.779767Z",
     "shell.execute_reply.started": "2024-04-05T06:08:40.769771Z"
    }
   },
   "source": [
    "<p style=\"font-size: 24px;text-align:center;\"><b>What is Google Gemma Model?</b></p>\n",
    "\n",
    "<div align=\"center\"><img style=\"height:40%; width:40%\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp\"/></div>\n",
    "\n",
    "<p style=\"text-align:center;font-size:18px;\"><b>Image </b><a href=\"https://blog.google/technology/developers/gemma-open-models/\"><b> src</b></a>\n",
    "\n",
    "<p style=\"font-size: 18px;\"><strong>Gemma Model</strong> is a family of open-source large language models created by Google AI. These models are known for being lightweight and powerful, achieving good performance on various tasks for their size. Here's a breakdown of what Gemma models are:</p>\n",
    "\n",
    "<ul style=\"font-size: 18px;\">\n",
    "    <li><strong>State-of-the-art:</strong> Compared to other models like Llama 2 and Mistral 7B, Gemma models perform competitively on benchmarks that measure knowledge, problem-solving abilities, and common sense reasoning.</li>\n",
    "    <li><strong>Lightweight:</strong> Gemma models are designed to be smaller and require less computing power than other large language models. This makes them more accessible for people who don't have access to powerful machines.</li>\n",
    "    <li><strong>Open-source:</strong> Anyone can access and use Gemma models, and researchers can even fine-tune them for specific tasks.</li>\n",
    "    <li><strong>Flexible:</strong> Gemma models work with various tools and frameworks, including TensorFlow, JAX, PyTorch, and Hugging Face Transformers. They can also run on different devices, from laptops to mobile phones.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size: 18px;\">Overall, Gemma models are a good option for developers and researchers who are looking for a powerful and versatile large language model that is easy to use and modify.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 24px;text-align:center;\"><b>Gemma Released Models</b></p>\n",
    "\n",
    "| Model Preset    | Tuned versions  | Preset                 |\n",
    "|-----------------|-----------------|------------------------|\n",
    "| `gemma_2b_en`| Pretrained      |    2B       |\n",
    "|  `gemma_instruct_2b_en`| Instruction tuned| 2B    |\n",
    "|`gemma_7b_en`| Pretrained    |  7B    |\n",
    "|  `gemma_instruct_7b_en` | Instruction tuned| 7B   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 24px;text-align:center;\"><b>Pretrained vs Instruction tuned</b></p>\n",
    "\n",
    "A pre-trained model and an instruction-tuned model are both starting points for creating a model that can perform a specific task. However, they differ in how the model learns that task:\n",
    "\n",
    "### **Pre-trained Model:**\n",
    "\n",
    "* **Training:** A pre-trained model is trained on a massive dataset of unlabeled text or data (like text or images) that covers a broad range of topics. This initial training helps the model develop a general understanding of language or the world. \n",
    "* **Focus:**  Think of it as learning the building blocks or foundational skills. It doesn't learn a specific task but develops a strong base for various tasks related to the type of data it's trained on. \n",
    "* **Benefits:** Pre-trained models are very efficient. They leverage the vast amount of data processed during pre-training, so you don't need to start from scratch when tackling a specific task. \n",
    "* **Drawback:** While versatile, they might not be perfect for a specific domain or task since the focus wasn't on that specific area.\n",
    "\n",
    "### **Instruction-Tuned Model:**\n",
    "\n",
    "* **Training:** An instruction-tuned model starts with a pre-trained model as a base. Then, it's further trained on a smaller dataset specifically designed for the desired task. This dataset often includes labeled examples with instructions or prompts about what the model should learn.  \n",
    "* **Focus:** This additional training refines the model's understanding to the specific task. It's like taking those building blocks and using them to construct something specific. \n",
    "* **Benefits:** Instruction-tuned models can achieve higher accuracy on a specific task compared to a pre-trained model used directly. \n",
    "* **Drawback:**  Tuning requires a task-specific dataset, and the success depends on the quality and size of that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 24px;text-align:center;\"><b>How to Access Google's Gemma Model?</b></p>\n",
    "\n",
    "<div align=\"center\"><img style=\"max-width:720px;\" src=\"https://i.ibb.co/VBLJ3tN/Screenshot-from-2024-04-04-23-43-49.png\" /></div>\n",
    "\n",
    "- Go to <a href=\"https://www.kaggle.com/models/google/gemma?utm_medium=kagglecomp&utm_source=kagglecompetition1&utm_campaign=models-gemmalaunch\"><b>Gemma,</b></a> scroll down and give the consent.\n",
    "- You are now ready to use the Gemma Model.\n",
    "- Click on Create Notebook\n",
    "- Click on Input\n",
    "<div align=\"center\"><img style=\"width:50%;max-width:240px;\" src=\"https://i.ibb.co/2Sxf1K1/Screenshot-from-2024-04-05-00-36-43.png\" alt=\"Screenshot-from-2024-04-05-00-36-43\" border=\"0\"></div>\n",
    "- Click on Add Input\n",
    "- Click on Gemma Model\n",
    "<div align=\"center\"><img style=\"width:50%;max-width:240px;\" src=\"https://i.ibb.co/3mHgJbD/Screenshot-from-2024-04-05-00-37-11.png\" alt=\"Screenshot-from-2024-04-05-00-37-11\" border=\"0\"></div>\n",
    "<br/>\n",
    "- Now selected Model will be visible in the Input Section\n",
    "- Select the Accelerator on which you want to run your model.\n",
    "<div align=\"center\"><img style=\"width:50%;max-width:240px;\" src=\"https://i.ibb.co/XFPrBst/Screenshot-from-2024-04-05-00-50-19.png\" alt=\"Screenshot-from-2024-04-05-00-50-19\" border=\"0\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Training the model**\n",
    "\n",
    "## **Install dependencies**\n",
    "### Install Keras, KerasNLP, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:16:56.042998Z",
     "iopub.status.busy": "2024-04-12T16:16:56.041938Z",
     "iopub.status.idle": "2024-04-12T16:18:48.355638Z",
     "shell.execute_reply": "2024-04-12T16:18:48.35436Z",
     "shell.execute_reply.started": "2024-04-12T16:16:56.042962Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q bitsandbytes\n",
    "%pip install -q transformers\n",
    "%pip install -q peft\n",
    "%pip install -q accelerate\n",
    "%pip install -q trl\n",
    "%pip install -q torch\n",
    "%pip install -q qdrant-client langchain pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load all libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:20:20.808177Z",
     "iopub.status.busy": "2024-04-12T16:20:20.807468Z",
     "iopub.status.idle": "2024-04-12T16:20:20.817925Z",
     "shell.execute_reply": "2024-04-12T16:20:20.817027Z",
     "shell.execute_reply.started": "2024-04-12T16:20:20.808146Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below configures a large language model (LLM) for inference with quantization techniques for efficiency. Here's a breakdown of what each part does:\n",
    "\n",
    "**Model Path and Quantization Configuration**\n",
    "\n",
    "1. **Model Path:** The `model` variable stores the path to a pre-trained causal language model (likely a 2-billion parameter model) on Kaggle Datasets.\n",
    "\n",
    "2. **BitsAndBytesConfig:** The `bnbConfig` object defines the configuration for quantization using the BitsAndBytes library. Here are the key arguments:\n",
    "    * `load_in_4bit (bool, optional)`: This argument enables 4-bit quantization, reducing memory usage by approximately fourfold compared to the original model.\n",
    "    * `bnb_4bit_quant_type (str, optional)`: This parameter specifies the type of 4-bit quantization to use. Here, it's set to `\"nf4\"`, a specific quantization format supported by BitsAndBytes.\n",
    "    * `bnb_4bit_compute_dtype (torch.dtype, optional)`: This argument defines the data type used for computations during inference. Here, it's set to `torch.bfloat16`, a lower-precision format that can improve speed on compatible hardware.\n",
    "\n",
    "**Loading Tokenizer and Model with Quantization**\n",
    "\n",
    "1. **AutoTokenizer:** The `AutoTokenizer.from_pretrained` function loads the tokenizer associated with the pre-trained model at the specified path (`model`). The `quantization_config` argument is crucial here. It tells the tokenizer to consider the quantization information (e.g., potential padding changes) while processing text.\n",
    "\n",
    "2. **AutoModelForCausalLM:** Similarly, `AutoModelForCausalLM.from_pretrained` loads the actual LLM model from the path (`model`). Again, the `device_map=\"auto\"` argument allows automatic device placement (CPU or GPU) and the `quantization_config` ensures the model is loaded with the 4-bit quantization configuration.\n",
    "\n",
    "**Overall, this code snippet aims to achieve two goals:**\n",
    "\n",
    "* **Load a pre-trained LLM:** It retrieves a pre-trained causal language model from the specified path.\n",
    "* **Enable Quantization for Efficiency:** By using the `BitsAndBytesConfig` and arguments during loading, the code configures the tokenizer and model to leverage 4-bit quantization for memory reduction and potentially faster inference on compatible hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><strong>Know More about <a href=\"https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2\">4-bit quantization</a></strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:26:32.5139Z",
     "iopub.status.busy": "2024-04-12T16:26:32.512981Z",
     "iopub.status.idle": "2024-04-12T16:26:38.728862Z",
     "shell.execute_reply": "2024-04-12T16:26:38.727951Z",
     "shell.execute_reply.started": "2024-04-12T16:26:32.513866Z"
    }
   },
   "outputs": [],
   "source": [
    "model = \"/kaggle/input/gemma/transformers/2b-it/2\"\n",
    "\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, quantization_config=bnbConfig, device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map = \"auto\",\n",
    "    quantization_config=bnbConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Model using a prompt**\n",
    "### The code below demonstrates how to use a large language model (LLM) for creative text generation. Here's a breakdown of what each part does:\n",
    "\n",
    "**Creating the Prompt**\n",
    "\n",
    "1. **System Response:** The code defines a variable `system` containing a message praising your Python coding skills.\n",
    "2. **User Request:** The `user` variable specifies the request to \"Write a Python code to display text in a star pattern.\"\n",
    "3. **Prompt Construction:** The `prompt` variable combines the system response, user request, and an AI response placeholder using f-strings.\n",
    "\n",
    "**Tokenization and Model Input Preparation**\n",
    "\n",
    "1. **Tokenizer:** The `tokenizer` likely refers to a pre-trained tokenizer function from the Hugging Face Transformers library. It converts the text in the prompt into numerical representations suitable for the LLM.\n",
    "2. **Tensor Conversion:** `.to(\"cuda\")` converts the tokenized prompt into a PyTorch tensor and moves it to the GPU (if available) for faster processing.\n",
    "\n",
    "**Model Generation**\n",
    "\n",
    "1. **Model Generation:** The `model.generate` function utilizes the LLM to generate text following the prompt. The provided arguments specify:\n",
    "    * `inputs`: The tokenized prompt as input.\n",
    "    * `num_return_sequences`: Set to 1, indicating only one generated sequence is desired.\n",
    "    * `max_new_tokens`: Limits the maximum number of tokens the model generates to 1000.\n",
    "\n",
    "**Decoding and Output**\n",
    "\n",
    "1. **Decoding:** The `tokenizer.decode` function converts the generated token sequence back into human-readable text.\n",
    "2. **Splitting and Markdown:** The code splits the generated text by \"AI:\" to extract the AI's response. Finally, it wraps the response in a Markdown object, likely for formatting purposes (not shown in the provided code).\n",
    "\n",
    "**Overall Functionality:**\n",
    "\n",
    "This code snippet simulates a conversation where the user asks for Python code for a star pattern, and the LLM generates the code using the prompt and its knowledge.\n",
    "\n",
    "**Note:** The actual Python code for generating a star pattern is not included here, but the LLM would likely generate the code based on its training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:29:36.53222Z",
     "iopub.status.busy": "2024-04-12T16:29:36.53182Z",
     "iopub.status.idle": "2024-04-12T16:29:41.243841Z",
     "shell.execute_reply": "2024-04-12T16:29:41.243094Z",
     "shell.execute_reply.started": "2024-04-12T16:29:36.532189Z"
    }
   },
   "outputs": [],
   "source": [
    "system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n",
    "user = \"Write a Python code to display text in a star pattern.\"\n",
    "\n",
    "prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=1000)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "Markdown(text.split(\"AI:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Fine Tune Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:11.18295Z",
     "iopub.status.busy": "2024-04-12T16:30:11.182031Z",
     "iopub.status.idle": "2024-04-12T16:30:11.350087Z",
     "shell.execute_reply": "2024-04-12T16:30:11.349185Z",
     "shell.execute_reply.started": "2024-04-12T16:30:11.182918Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\")\n",
    "dataset = Dataset.from_pandas(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a formatting function for the model output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:13.995259Z",
     "iopub.status.busy": "2024-04-12T16:30:13.994511Z",
     "iopub.status.idle": "2024-04-12T16:30:13.999965Z",
     "shell.execute_reply": "2024-04-12T16:30:13.998997Z",
     "shell.execute_reply.started": "2024-04-12T16:30:13.995226Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "    line = template.format(instruction=example['Question'], response=example['Answer'])\n",
    "    return [line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:20.191939Z",
     "iopub.status.busy": "2024-04-12T16:30:20.191008Z",
     "iopub.status.idle": "2024-04-12T16:30:20.196102Z",
     "shell.execute_reply": "2024-04-12T16:30:20.195089Z",
     "shell.execute_reply.started": "2024-04-12T16:30:20.191905Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is WANDB?**\n",
    "\n",
    "WANDB is a cloud platform for experiment tracking specifically designed for machine learning. It provides functionalities like:\n",
    "\n",
    "* **Logging** training metrics, parameters, and visualizations.\n",
    "* **Version control** for your machine learning experiments.\n",
    "* **Sweeping** hyperparameters to find the best performing configuration.\n",
    "* **Collaboration** features to share and discuss experiments with your team.\n",
    "\n",
    "## **Why disable WANDB?**\n",
    "\n",
    "There are a few reasons why you might want to disable WANDB:\n",
    "\n",
    "* **You don't need experiment tracking:** If your code is a simple experiment or you're not interested in tracking the results, disabling WANDB can reduce overhead.\n",
    "* **Privacy concerns:** WANDB logs your experiment data to the cloud. If your data is sensitive, you might not want to upload it.\n",
    "* **Troubleshooting errors:**  Sometimes errors can arise from WANDB itself. Disabling it can help isolate the issue.\n",
    "\n",
    "## **Setting the `WANDB_DISABLED` environment variable**\n",
    "\n",
    "We sets an environment variable called `WANDB_DISABLED` to `\"true\"`. This tells the WANDB library to not initialize itself, effectively disabling it for current script.\n",
    "\n",
    "## **In summary:**\n",
    "\n",
    "* WANDB is a useful tool for experiment tracking in machine learning.\n",
    "* You might disable WANDB if you don't need experiment tracking or for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:28.576212Z",
     "iopub.status.busy": "2024-04-12T16:30:28.575806Z",
     "iopub.status.idle": "2024-04-12T16:30:28.58148Z",
     "shell.execute_reply": "2024-04-12T16:30:28.580387Z",
     "shell.execute_reply.started": "2024-04-12T16:30:28.576182Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above defines a configuration object called `lora_config` for a technique called LoRA (Low-Rank Adaptation). Here's a breakdown of what each parameter does:\n",
    "\n",
    "**LoRA - Low-Rank Adaptation**\n",
    "\n",
    "LoRA is a technique used to fine-tune large language models (LLMs) more efficiently. It allows you to adapt pre-trained models to new tasks with minimal memory and computational cost compared to traditional fine-tuning.\n",
    "\n",
    "**LoraConfig Parameters:**\n",
    "\n",
    "* **r (int):** This parameter defines the rank of the low-rank decomposition used in LoRA. It controls the trade-off between accuracy and memory usage. A lower value of `r` uses less memory but might lead to slightly lower accuracy. The default value is typically 8, as set in our code.\n",
    "\n",
    "* **target_modules (List[str]):** This list specifies the Transformer layers where LoRA will be applied. The provided configuration targets several key projection layers within the Transformer architecture:\n",
    "    * `q_proj`: Query projection\n",
    "    * `o_proj`: Output projection\n",
    "    * `k_proj`: Key projection\n",
    "    * `v_proj`: Value projection\n",
    "    * `gate_proj`: Gate projection (used in attention layers)\n",
    "    * `up_proj`: Upsampling projection (used in some encoder-decoder architectures)\n",
    "    * `down_proj`: Downsampling projection (used in some encoder-decoder architectures)\n",
    "\n",
    "By applying LoRA to these projection layers, the model can learn task-specific adaptations without modifying the original large model weights significantly.\n",
    "\n",
    "* **task_type (str, optional):** This parameter specifies the type of task you're fine-tuning the model for. While not used in this specific configuration, some libraries might leverage this information to optimize LoRA for specific task categories (e.g., \"CAUSAL_LM\" for causal language modeling).\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "This configuration defines how LoRA will be applied to a pre-trained model for fine-tuning. It specifies the rank of the decomposition (memory usage) and the target layers within the Transformer architecture where LoRA will be used to adapt the model to a new task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:36.765507Z",
     "iopub.status.busy": "2024-04-12T16:30:36.764624Z",
     "iopub.status.idle": "2024-04-12T16:30:39.52798Z",
     "shell.execute_reply": "2024-04-12T16:30:39.527036Z",
     "shell.execute_reply.started": "2024-04-12T16:30:36.765472Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above creates an instance of `SFTTrainer` from the Transformers library specifically designed for supervised fine-tuning (SFT) tasks. Here's a breakdown of what each part does:\n",
    "\n",
    "**SFTTrainer for Supervised Fine-Tuning**\n",
    "\n",
    "This code utilizes `SFTTrainer` to fine-tune a pre-trained model (`model`) on a specific training dataset (`dataset`). It's designed for tasks where you have labeled data and want to adapt the model for a new purpose.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "* **model (PreTrainedModel):** This argument specifies the pre-trained model you want to fine-tune.\n",
    "\n",
    "* **train_dataset (Dataset):** This argument points to the training dataset you'll use for fine-tuning. The dataset should be formatted appropriately for the task.\n",
    "\n",
    "* **max_seq_length (int):** This parameter defines the maximum sequence length allowed in the training data. Sequences exceeding this length will be truncated.\n",
    "\n",
    "* **args (TrainingArguments):** This argument is an instance of `TrainingArguments` that defines various hyperparameters for the training process. Here are some notable arguments within `args`:\n",
    "    * `per_device_train_batch_size (int)`: Sets the batch size per device (GPU/TPU) during training. Here, it's set to 1, which is a small batch size commonly used with gradient accumulation.\n",
    "    * `gradient_accumulation_steps (int)`: This parameter allows accumulating gradients over several batches before updating the model weights. Here, it's set to 4, effectively increasing the effective batch size.\n",
    "    * `warmup_steps (int)`: This defines the number of warmup steps where the learning rate is gradually increased from 0 to its full value. Here, it's set to 2.\n",
    "    * `max_steps (int)`: This parameter specifies the total number of training steps. Here, it's set to 50, which might be a short training run for fine-tuning depending on your dataset size and task complexity.\n",
    "    * `learning_rate (float)`: This sets the learning rate for the optimizer. Here, it's set to 2e-4, which is a common starting point for fine-tuning.\n",
    "    * `fp16 (bool)`: Enables training using 16-bit floating-point precision (mixed precision) for faster training with minimal accuracy loss (if supported by your hardware).\n",
    "    * `logging_steps (int)`: Defines how often training metrics are logged during training. Here, it's set to 1, logging metrics for every step. \n",
    "    * `output_dir (str)`: Specifies the directory where training outputs (model checkpoints, logs, etc.) will be saved. Here, it's set to \"outputs\".\n",
    "    * `optim (str)`: Defines the optimizer used for training. Here, it's set to \"paged_adamw_8bit\", which is likely an optimizer with specific memory optimizations. \n",
    "\n",
    "* **peft_config (LoraConfig):** This argument is likely referencing the `lora_config` you defined earlier. It provides the configuration for LoRA (Low-Rank Adaptation), which helps fine-tune the model more efficiently.\n",
    "\n",
    "* **formatting_func (Callable):** This argument (if provided) specifies a custom function for formatting the training data before feeding it to the model. This allows for specific pre-processing steps tailored to your task.\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "This code snippet configures and initializes an `SFTTrainer` for fine-tuning a pre-trained model with LoRA for memory efficiency. The training hyperparameters are set within the `TrainingArguments` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:30:46.830615Z",
     "iopub.status.busy": "2024-04-12T16:30:46.829703Z",
     "iopub.status.idle": "2024-04-12T16:31:23.158109Z",
     "shell.execute_reply": "2024-04-12T16:31:23.157157Z",
     "shell.execute_reply.started": "2024-04-12T16:30:46.830581Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test the Fine-Tuned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:31:37.161886Z",
     "iopub.status.busy": "2024-04-12T16:31:37.1611Z",
     "iopub.status.idle": "2024-04-12T16:31:47.793398Z",
     "shell.execute_reply": "2024-04-12T16:31:47.792459Z",
     "shell.execute_reply.started": "2024-04-12T16:31:37.161855Z"
    }
   },
   "outputs": [],
   "source": [
    "system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n",
    "question =system + \"What is the difference between a variable and an object\"\n",
    "\n",
    "prompt = f\"Question: {question} \\n Answer: \"\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=512)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "Markdown(text.split(\"Answer:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Retrieval Augment Generation (RAG)**\n",
    "```Retrieval Augmented Generation (RAG)``` is a paradigm in language model architecture that integrates both retrieval and generation processes to enhance the model's understanding and response capabilities. In essence, it combines the strengths of retrieval-based models, which excel at accessing and utilizing external knowledge sources, with generative models, which can generate novel and contextually relevant responses.\n",
    "\n",
    "The primary benefit of RAG in large language models (LLMs) is its ability to leverage external knowledge sources during the generation process. By retrieving relevant information from a predefined knowledge base or corpus, the model can augment its understanding of the input context and produce more accurate and informative responses. This approach not only improves the coherence and relevance of generated text but also enables the model to incorporate real-world knowledge and factual accuracy into its outputs.\n",
    "\n",
    "RAG aims to achieve several key objectives:\n",
    "\n",
    "1. **Enhanced Contextual Understanding:** By retrieving relevant information from external sources, RAG can better understand the context of a given prompt or query, leading to more contextually appropriate responses.\n",
    "\n",
    "2. **Improved Content Quality:** Integrating external knowledge sources allows RAG to generate content that is more accurate, informative, and relevant to the input context, enhancing the overall quality of generated text.\n",
    "\n",
    "3. **Factually Accurate Responses:** By accessing external knowledge bases, RAG can ensure that its responses are factually accurate and grounded in real-world information, reducing the likelihood of generating misleading or incorrect information.\n",
    "\n",
    "The workflow of RAG typically involves the following steps:\n",
    "\n",
    "1. **Retrieval:** The model first retrieves relevant information from a knowledge base or corpus based on the input prompt or query. This retrieval process aims to identify key facts, concepts, or contextually relevant information to inform the generation process.\n",
    "\n",
    "2. **Augmentation:** The retrieved information is then used to augment the model's understanding of the input context. By incorporating this external knowledge, the model can generate more informed and contextually appropriate responses.\n",
    "\n",
    "3. **Generation:** Finally, the model generates a response based on the augmented understanding of the input context, leveraging both the original prompt and the retrieved information to produce a coherent and relevant output.\n",
    "\n",
    "The necessity of using RAG lies in its ability to address the limitations of traditional generative models, such as lack of factual accuracy and coherence in responses. By integrating retrieval-based mechanisms, RAG can access external knowledge sources to enhance its understanding of the input context, leading to more accurate, informative, and contextually relevant generated text. This approach is particularly valuable in tasks requiring a deep understanding of complex topics or access to large knowledge bases, such as question answering, dialogue generation, and content summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load documents for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:32:17.477606Z",
     "iopub.status.busy": "2024-04-12T16:32:17.476936Z",
     "iopub.status.idle": "2024-04-12T16:33:30.695365Z",
     "shell.execute_reply": "2024-04-12T16:33:30.694491Z",
     "shell.execute_reply.started": "2024-04-12T16:32:17.477573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a PyPDFDirectoryLoader object with the specified directory path\n",
    "pdf_loader = PyPDFDirectoryLoader(\"/kaggle/input/knowledge-base\")\n",
    "\n",
    "# Load PDF documents from the specified directory\n",
    "pdfs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:33:30.697181Z",
     "iopub.status.busy": "2024-04-12T16:33:30.696847Z",
     "iopub.status.idle": "2024-04-12T16:33:36.216502Z",
     "shell.execute_reply": "2024-04-12T16:33:36.215518Z",
     "shell.execute_reply.started": "2024-04-12T16:33:30.697154Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the HuggingFaceEmbeddings class, \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # This argument specifies the pre-trained model name to be used for generating embeddings.\n",
    "    # Here, \"sentence-transformers/all-mpnet-base-v2\" is a pre-trained sentence transformer model \n",
    "    # from the Sentence Transformers library (not Transformers).\n",
    "    # Sentence transformer models are specifically trained to generate meaningful representations \n",
    "    # of sentences that capture semantic similarity.\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\n",
    "    # This argument is likely specific to the HuggingFaceEmbeddings class and might \n",
    "    # not be present in the base Transformers library.\n",
    "    # It sets the device to \"cuda\" to leverage the GPU for faster processing if available.\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:38:02.322858Z",
     "iopub.status.busy": "2024-04-12T16:38:02.322428Z",
     "iopub.status.idle": "2024-04-12T16:38:03.569279Z",
     "shell.execute_reply": "2024-04-12T16:38:03.568357Z",
     "shell.execute_reply.started": "2024-04-12T16:38:02.322828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a RecursiveCharacterTextSplitter object with specified parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split documents into chunks using the RecursiveCharacterTextSplitter\n",
    "all_splits = text_splitter.split_documents(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:38:14.243736Z",
     "iopub.status.busy": "2024-04-12T16:38:14.243308Z",
     "iopub.status.idle": "2024-04-12T16:41:50.691784Z",
     "shell.execute_reply": "2024-04-12T16:41:50.690827Z",
     "shell.execute_reply.started": "2024-04-12T16:38:14.243707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Qdrant collection from the document splits\n",
    "# For storing and searching document information we use a vector database called Qdrant. \n",
    "\n",
    "qdrant_collection = Qdrant.from_documents(\n",
    "    all_splits,                # List of document splits\n",
    "    embeddings,                # HuggingFaceEmbeddings object for generating embeddings\n",
    "    location=\":memory:\",       # Location to store the collection (in memory)\n",
    "    collection_name=\"all_documents\"  # Name of the Qdrant collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:41:50.693591Z",
     "iopub.status.busy": "2024-04-12T16:41:50.693302Z",
     "iopub.status.idle": "2024-04-12T16:41:50.698005Z",
     "shell.execute_reply": "2024-04-12T16:41:50.697098Z",
     "shell.execute_reply.started": "2024-04-12T16:41:50.693567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = qdrant_collection.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:43:25.020627Z",
     "iopub.status.busy": "2024-04-12T16:43:25.020219Z",
     "iopub.status.idle": "2024-04-12T16:43:25.026795Z",
     "shell.execute_reply": "2024-04-12T16:43:25.02572Z",
     "shell.execute_reply.started": "2024-04-12T16:43:25.020598Z"
    }
   },
   "outputs": [],
   "source": [
    "# This code creates a pipeline for text generation using a pre-trained model (model) \n",
    "# and its tokenizer (tokenizer). It leverages mixed precision (torch.bfloat16) \n",
    "# for potentially faster inference and limits generated text to 512 tokens.\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs = {\"torch.dtype\": torch.bfloat16},\n",
    "    max_new_tokens=512    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's a breakdown of the code snippet below and the parameters used for text generation:\n",
    "\n",
    "**1. Prompt Creation (`pipeline.tokenizer.apply_chat_template`)**\n",
    "\n",
    "* **Function:** This part uses a function (likely) provided by the text-generation pipeline to specifically format the conversation history (`messages`) into a prompt suitable for the LLM. \n",
    "* **Arguments:**\n",
    "    * `messages (List[Dict])`: This argument is a list of dictionaries representing the conversation history. Each dictionary likely contains keys like \"role\" (indicating user or system) and \"content\" (the actual text).\n",
    "    * `tokenize (bool, optional)`: This argument is likely set to `False` here, indicating that the tokenizer should not be applied at this stage. The conversation history might already be pre-processed text.\n",
    "    * `add_generation_prompt (bool, optional)`: This argument is likely set to `True`, instructing the function to add a prompt at the beginning specifically designed for chat-like text generation tasks. The exact format of this prompt might be specific to the pipeline implementation.\n",
    "\n",
    "**2. Text Generation with Pipeline (`pipeline`)**\n",
    "\n",
    "* **Function:** This line calls the text-generation pipeline itself to generate text that continues the conversation based on the provided prompt.\n",
    "* **Arguments:**\n",
    "    * `prompt (str)`: This argument is the crucial prompt created in the previous step, containing the formatted conversation history and potentially a generation prompt.\n",
    "    * `max_new_tokens (int, optional)`: This argument limits the number of tokens (words) the LLM can generate to 512 in this case.\n",
    "    * `add_special_tokens (bool, optional)`: This argument, set to `True` here, instructs the pipeline to add special tokens (like start/end of sequence tokens) to the prompt before feeding it to the LLM.\n",
    "    * `do_sample (bool, optional)`: Set to `True` here, enabling random sampling during generation, which can introduce some variation in the response compared to greedy decoding.\n",
    "    * `temperature (float, optional)`: This argument controls the randomness of the generated text. A temperature of 1.0 samples more uniformly from all possibilities, while lower values like 0.7 (used here) favor higher probability tokens, resulting in a more conservative response.\n",
    "    * `top_k (int, optional)`: This argument restricts the generation to only consider the top k most likely tokens at each step, potentially reducing the likelihood of going off on tangents. Here, it's set to 10.\n",
    "    * `top_p (float, optional)`: This argument controls the sampling process by favoring the top p probability mass of the distribution (considering the top k tokens). A value of 0.95 (used here) indicates a preference for high-probability tokens, influencing the creativity of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T16:58:58.752723Z",
     "iopub.status.busy": "2024-04-12T16:58:58.752279Z",
     "iopub.status.idle": "2024-04-12T16:59:35.535669Z",
     "shell.execute_reply": "2024-04-12T16:59:35.534733Z",
     "shell.execute_reply.started": "2024-04-12T16:58:58.752677Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between a variable and an object\"\n",
    "\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    add_special_tokens=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=10,\n",
    "    top_p=0.95\n",
    ")\n",
    "Markdown(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below combines a large language model (LLM) for text generation with a retrieval system for building a more informative response system. Here's a breakdown of what each part does:\n",
    "\n",
    "**1. Creating a Custom LLM Pipeline (`gemma_llm`)**\n",
    "\n",
    "* **HuggingFacePipeline:** This line likely wraps the existing text-generation pipeline (`pipeline`) from the previous steps into a custom `HuggingFacePipeline` class. This might provide additional functionalities or a more convenient way to interact with the pipeline.\n",
    "* **Model Kwargs:** It sets a default argument (`temperature=0.7`) for the custom pipeline. This argument controls the randomness of the generated text during text generation (explained earlier).\n",
    "\n",
    "**2. Building a RetrievalQA Object (`qa`)**\n",
    "\n",
    "* **RetrievalQA.from_chain_type:** This line creates a `RetrievalQA` object, likely from a custom library or framework that combines retrieval and question answering functionalities.\n",
    "* **Arguments:**\n",
    "    * `llm (TextGenerationPipeline)`: This argument takes the `gemma_llm` object, which provides the text-generation capabilities.\n",
    "    * `chain_type (str)`: This argument is set to `\"stuff\"`, which might be a specific type of retrieval-LM chain supported by the `RetrievalQA` class. The exact functionality of `\"stuff\"` depends on the library's implementation.\n",
    "    * `retriever (RetrievalInterface)`: This argument likely refers to a separate `retriever` object (not shown here) that handles retrieving relevant information from a knowledge base or document store based on the user's query.\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "This code combines the text-generation capabilities of the LLM with a retrieval system. The `RetrievalQA` object likely leverages the retrieved information (through the `retriever` object) to inform the LLM's text generation process, potentially leading to more comprehensive and informative responses to user queries. The specific details of how retrieval and text generation are chained together depend on the `RetrievalQA` implementation and the `\"stuff\"` chain type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T17:09:48.257806Z",
     "iopub.status.busy": "2024-04-12T17:09:48.257367Z",
     "iopub.status.idle": "2024-04-12T17:09:48.26507Z",
     "shell.execute_reply": "2024-04-12T17:09:48.263797Z",
     "shell.execute_reply.started": "2024-04-12T17:09:48.257772Z"
    }
   },
   "outputs": [],
   "source": [
    "gemma_llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"add_special_tokens\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    ")\n",
    "# Create a RetrievalQA object\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=gemma_llm,  # Pass the text-generation pipeline object\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever  # retriever object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T17:18:47.065635Z",
     "iopub.status.busy": "2024-04-12T17:18:47.064533Z",
     "iopub.status.idle": "2024-04-12T17:19:23.871761Z",
     "shell.execute_reply": "2024-04-12T17:19:23.87086Z",
     "shell.execute_reply.started": "2024-04-12T17:18:47.06559Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"Write in detail about python\"\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True, truncation=True)\n",
    "result = qa.invoke(prompt)\n",
    "Markdown(result['result'].split('Helpful Answer:')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4607812,
     "sourceId": 7856134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4616621,
     "sourceId": 7970419,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5388,
     "sourceId": 11372,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
