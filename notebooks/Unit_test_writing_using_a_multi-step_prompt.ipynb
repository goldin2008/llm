{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test writing using a multi-step prompt\n",
    "\n",
    "Complex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT and then feeds that output text back into subsequent prompts. This can help in cases where you want GPT to reason things out before answering, or brainstorm a plan before executing it.\n",
    "\n",
    "In this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n",
    "\n",
    "1. **Explain**: Given a Python function, we ask GPT to explain what the function is doing and why.\n",
    "2. **Plan**: We ask GPT to plan a set of unit tests for the function.\n",
    "    - If the plan is too short, we ask GPT to elaborate with more ideas for unit tests.\n",
    "3. **Execute**: Finally, we instruct GPT to write unit tests that cover the planned cases.\n",
    "\n",
    "The code example illustrates a few embellishments on the chained, multi-step prompt:\n",
    "\n",
    "- Conditional branching (e.g., asking for elaboration only if the first plan is too short)\n",
    "- The choice of different models for different steps\n",
    "- A check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's `ast` module)\n",
    "- Streaming output so that you can start reading the output before it's fully generated (handy for long, multi-step outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed to run the code in this notebook\n",
    "import ast  # used for detecting whether generated Python code is valid\n",
    "import openai  # used for calling the OpenAI API\n",
    "\n",
    "color_prefix_by_role = {\n",
    "    \"system\": \"\\033[0m\",  # gray\n",
    "    \"user\": \"\\033[0m\",  # gray\n",
    "    \"assistant\": \"\\033[92m\",  # green\n",
    "}\n",
    "\n",
    "\n",
    "def print_messages(messages, color_prefix_by_role=color_prefix_by_role) -> None:\n",
    "    \"\"\"Prints messages sent to or from GPT.\"\"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        color_prefix = color_prefix_by_role[role]\n",
    "        content = message[\"content\"]\n",
    "        print(f\"{color_prefix}\\n[{role}]\\n{content}\")\n",
    "\n",
    "\n",
    "def print_message_delta(delta, color_prefix_by_role=color_prefix_by_role) -> None:\n",
    "    \"\"\"Prints a chunk of messages streamed back from GPT.\"\"\"\n",
    "    if \"role\" in delta:\n",
    "        role = delta[\"role\"]\n",
    "        color_prefix = color_prefix_by_role[role]\n",
    "        print(f\"{color_prefix}\\n[{role}]\\n\", end=\"\")\n",
    "    elif \"content\" in delta:\n",
    "        content = delta[\"content\"]\n",
    "        print(content, end=\"\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# example of a function that uses a multi-step prompt to write unit tests\n",
    "def unit_tests_from_function(\n",
    "    function_to_test: str,  # Python function to test, as a string\n",
    "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
    "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
    "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
    "    explain_model: str = \"gpt-3.5-turbo\",  # model used to generate text plans in step 1\n",
    "    plan_model: str = \"gpt-3.5-turbo\",  # model used to generate text plans in steps 2 and 2b\n",
    "    execute_model: str = \"gpt-3.5-turbo\",  # model used to generate code in step 3\n",
    "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
    "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
    ") -> str:\n",
    "    \"\"\"Returns a unit test for a given Python function, using a 3-step GPT prompt.\"\"\"\n",
    "\n",
    "    # Step 1: Generate an explanation of the function\n",
    "\n",
    "    # create a markdown-formatted message that asks GPT to explain the function, formatted as a bullet list\n",
    "    explain_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\",\n",
    "    }\n",
    "    explain_user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
    "\n",
    "```python\n",
    "{function_to_test}\n",
    "```\"\"\",\n",
    "    }\n",
    "    explain_messages = [explain_system_message, explain_user_message]\n",
    "    if print_text:\n",
    "        print_messages(explain_messages)\n",
    "\n",
    "    explanation_response = openai.ChatCompletion.create(\n",
    "        model=explain_model,\n",
    "        messages=explain_messages,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    explanation = \"\"\n",
    "    for chunk in explanation_response:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if print_text:\n",
    "            print_message_delta(delta)\n",
    "        if \"content\" in delta:\n",
    "            explanation += delta[\"content\"]\n",
    "    explain_assistant_message = {\"role\": \"assistant\", \"content\": explanation}\n",
    "\n",
    "    # Step 2: Generate a plan to write a unit test\n",
    "\n",
    "    # Asks GPT to plan out cases the units tests should cover, formatted as a bullet list\n",
    "    plan_user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"A good unit test suite should aim to:\n",
    "- Test the function's behavior for a wide range of possible inputs\n",
    "- Test edge cases that the author may not have foreseen\n",
    "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
    "- Be easy to read and understand, with clean code and descriptive names\n",
    "- Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\"\"\",\n",
    "    }\n",
    "    plan_messages = [\n",
    "        explain_system_message,\n",
    "        explain_user_message,\n",
    "        explain_assistant_message,\n",
    "        plan_user_message,\n",
    "    ]\n",
    "    if print_text:\n",
    "        print_messages([plan_user_message])\n",
    "    plan_response = openai.ChatCompletion.create(\n",
    "        model=plan_model,\n",
    "        messages=plan_messages,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    plan = \"\"\n",
    "    for chunk in plan_response:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if print_text:\n",
    "            print_message_delta(delta)\n",
    "        if \"content\" in delta:\n",
    "            plan += delta[\"content\"]\n",
    "    plan_assistant_message = {\"role\": \"assistant\", \"content\": plan}\n",
    "\n",
    "    # Step 2b: If the plan is short, ask GPT to elaborate further\n",
    "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
    "    num_bullets = max(plan.count(\"\\n-\"), plan.count(\"\\n*\"))\n",
    "    elaboration_needed = num_bullets < approx_min_cases_to_cover\n",
    "    if elaboration_needed:\n",
    "        elaboration_user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\"\"\",\n",
    "        }\n",
    "        elaboration_messages = [\n",
    "            explain_system_message,\n",
    "            explain_user_message,\n",
    "            explain_assistant_message,\n",
    "            plan_user_message,\n",
    "            plan_assistant_message,\n",
    "            elaboration_user_message,\n",
    "        ]\n",
    "        if print_text:\n",
    "            print_messages([elaboration_user_message])\n",
    "        elaboration_response = openai.ChatCompletion.create(\n",
    "            model=plan_model,\n",
    "            messages=elaboration_messages,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        elaboration = \"\"\n",
    "        for chunk in elaboration_response:\n",
    "            delta = chunk[\"choices\"][0][\"delta\"]\n",
    "            if print_text:\n",
    "                print_message_delta(delta)\n",
    "            if \"content\" in delta:\n",
    "                elaboration += delta[\"content\"]\n",
    "        elaboration_assistant_message = {\"role\": \"assistant\", \"content\": elaboration}\n",
    "\n",
    "    # Step 3: Generate the unit test\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT to complete a unit test\n",
    "    package_comment = \"\"\n",
    "    if unit_test_package == \"pytest\":\n",
    "        package_comment = \"# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
    "    execute_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\",\n",
    "    }\n",
    "    execute_user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Using Python and the `{unit_test_package}` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
    "\n",
    "```python\n",
    "# imports\n",
    "import {unit_test_package}  # used for our unit tests\n",
    "{{insert other imports as needed}}\n",
    "\n",
    "# function to test\n",
    "{function_to_test}\n",
    "\n",
    "# unit tests\n",
    "{package_comment}\n",
    "{{insert unit test code here}}\n",
    "```\"\"\",\n",
    "    }\n",
    "    execute_messages = [\n",
    "        execute_system_message,\n",
    "        explain_user_message,\n",
    "        explain_assistant_message,\n",
    "        plan_user_message,\n",
    "        plan_assistant_message,\n",
    "    ]\n",
    "    if elaboration_needed:\n",
    "        execute_messages += [elaboration_user_message, elaboration_assistant_message]\n",
    "    execute_messages += [execute_user_message]\n",
    "    if print_text:\n",
    "        print_messages([execute_system_message, execute_user_message])\n",
    "\n",
    "    execute_response = openai.ChatCompletion.create(\n",
    "        model=execute_model,\n",
    "        messages=execute_messages,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    execution = \"\"\n",
    "    for chunk in execute_response:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if print_text:\n",
    "            print_message_delta(delta)\n",
    "        if \"content\" in delta:\n",
    "            execution += delta[\"content\"]\n",
    "\n",
    "    # check the output for errors\n",
    "    code = execution.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax error in generated code: {e}\")\n",
    "        if reruns_if_fail > 0:\n",
    "            print(\"Rerunning...\")\n",
    "            return unit_tests_from_function(\n",
    "                function_to_test=function_to_test,\n",
    "                unit_test_package=unit_test_package,\n",
    "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
    "                print_text=print_text,\n",
    "                explain_model=explain_model,\n",
    "                plan_model=plan_model,\n",
    "                execute_model=execute_model,\n",
    "                temperature=temperature,\n",
    "                reruns_if_fail=reruns_if_fail\n",
    "                - 1,  # decrement rerun counter when calling again\n",
    "            )\n",
    "\n",
    "    # return the unit test as a string\n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "[system]\n",
      "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\n",
      "\u001b[0m\n",
      "[user]\n",
      "Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
      "\n",
      "```python\n",
      "\n",
      "from flask import Flask, session, jsonify, request\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "import os\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "import json\n",
      "\n",
      "\n",
      "\n",
      "#################Load config.json and get path variables\n",
      "with open('config.json','r') as f:\n",
      "    config = json.load(f) \n",
      "\n",
      "output_model_path = os.path.join(config['output_model_path']) \n",
      "test_data_path = os.path.join(config['test_data_path']) \n",
      "\n",
      "\n",
      "#################Function for model scoring\n",
      "def score_model():\n",
      "    #this function should take a trained model, load test data, and calculate an F1 score for the model relative to the test data\n",
      "    #it should write the result to the latestscore.txt file\n",
      "\n",
      "    # Take trained model\n",
      "    model_path = os.path.join(output_model_path, \"trainedmodel.pkl\")\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "\n",
      "    test_df = pd.read_csv(os.path.join(test_data_path, 'testdata.csv'))\n",
      "\n",
      "    y_test = test_df.pop('exited')\n",
      "    X_test = test_df.drop(['corporation'], axis=1)\n",
      "\n",
      "    y_pred = model.predict(X_test)\n",
      "    score = metrics.f1_score(y_test, y_pred)\n",
      "\n",
      "    score_path = os.path.join(output_model_path, \"latestscore.txt\")\n",
      "    with open(score_path, 'w') as f:\n",
      "        f.write(str(score))\n",
      "\n",
      "    return score\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = score_model()\n",
      "\n",
      "```\n",
      "\u001b[92m\n",
      "[assistant]\n",
      "This Python function is a script that loads a trained model and test data, calculates an F1 score for the model relative to the test data, and writes the result to a file called latestscore.txt. Here's what the code is doing in detail:\n",
      "\n",
      "- The code begins by importing Flask, session, jsonify, request, pandas, numpy, pickle, os, metrics, and train_test_split from the scikit-learn library.\n",
      "- The code then imports the contents of the config.json file, which contains the paths to the output model and test data.\n",
      "- The code defines a function called score_model() that will calculate the F1 score for the model relative to the test data.\n",
      "- The function begins by loading the trained model from the trainedmodel.pkl file using the pickle.load() method.\n",
      "- The function then loads the test data from the testdata.csv file using the pandas.read_csv() method.\n",
      "- The function separates the target variable (exited) from the feature variables in the test data using the pandas.DataFrame.pop() method and stores it in a variable called y_test.\n",
      "- The function drops the corporation column from the feature variables using the pandas.DataFrame.drop() method and stores the remaining columns in a variable called X_test.\n",
      "- The function uses the trained model to predict the target variable (exited) for the test data using the model.predict() method and stores the predictions in a variable called y_pred.\n",
      "- The function calculates the F1 score for the model relative to the test data using the metrics.f1_score() method and stores the result in a variable called score.\n",
      "- The function writes the F1 score to the latestscore.txt file using the open() method with the 'w' flag to write to the file and the write() method to write the score as a string.\n",
      "- Finally, the function returns the F1 score.\n",
      "- The code then checks if the script is being run as the main program using the if __name__ == \"__main__\": statement.\n",
      "- If the script is being run as the main program, it calls the score_model() function and stores the result in a variable called score.\u001b[0m\n",
      "[user]\n",
      "A good unit test suite should aim to:\n",
      "- Test the function's behavior for a wide range of possible inputs\n",
      "- Test edge cases that the author may not have foreseen\n",
      "- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
      "- Be easy to read and understand, with clean code and descriptive names\n",
      "- Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\n",
      "\u001b[92m\n",
      "[assistant]\n",
      "Here are some scenarios that the function should be able to handle, along with some examples:\n",
      "\n",
      "- Test that the function can load a trained model and test data from the specified paths\n",
      "    - Test that the function raises an exception if the model or test data cannot be loaded\n",
      "    - Test that the function raises an exception if the specified paths are invalid or do not exist\n",
      "- Test that the function can calculate the F1 score for the model relative to the test data\n",
      "    - Test that the function returns a valid F1 score for a given model and test data\n",
      "    - Test that the function raises an exception if the model or test data is invalid or missing required columns\n",
      "    - Test that the function raises an exception if the model or test data has missing or NaN values\n",
      "- Test that the function can write the F1 score to the latestscore.txt file\n",
      "    - Test that the function creates the latestscore.txt file if it does not exist\n",
      "    - Test that the function overwrites the latestscore.txt file if it already exists\n",
      "    - Test that the function raises an exception if it cannot write to the latestscore.txt file\n",
      "- Test that the function returns a valid F1 score\n",
      "    - Test that the function returns a float between 0 and 1\n",
      "    - Test that the function returns a score of 0 if all predictions are incorrect\n",
      "    - Test that the function returns a score of 1 if all predictions are correct\n",
      "    - Test that the function returns a score of 0.5 if the predictions are random\n",
      "- Test that the function is deterministic\n",
      "    - Test that the function returns the same score for the same model and test data\n",
      "    - Test that the function returns the same score regardless of the order of the test data\n",
      "    - Test that the function returns the same score regardless of the order of the columns in the test data\u001b[0m\n",
      "[user]\n",
      "In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\n",
      "\u001b[92m\n",
      "[assistant]\n",
      "Here are some rare or unexpected edge cases that the function should be able to handle, along with some examples:\n",
      "\n",
      "- Test that the function can handle very large or very small test data\n",
      "    - Test that the function can handle test data with millions of rows or columns\n",
      "    - Test that the function can handle test data with very small or very large values\n",
      "- Test that the function can handle missing or invalid config.json values\n",
      "    - Test that the function can handle missing or invalid output_model_path or test_data_path values in the config.json file\n",
      "    - Test that the function can handle a missing or invalid config.json file\n",
      "- Test that the function can handle unexpected or invalid model types\n",
      "    - Test that the function can handle models that are not trained with scikit-learn\n",
      "    - Test that the function can handle models that are not saved as pickle files\n",
      "    - Test that the function can handle models that are missing required attributes or methods\n",
      "- Test that the function can handle unexpected or invalid test data types\n",
      "    - Test that the function can handle test data that is not a pandas DataFrame\n",
      "    - Test that the function can handle test data with missing or invalid column names\n",
      "    - Test that the function can handle test data with missing or invalid target or feature columns\n",
      "- Test that the function can handle unexpected or invalid F1 score types\n",
      "    - Test that the function can handle F1 scores that are not floats\n",
      "    - Test that the function can handle F1 scores that are not between 0 and 1\n",
      "    - Test that the function can handle F1 scores that are NaN or infinite\u001b[0m\n",
      "[system]\n",
      "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\n",
      "\u001b[0m\n",
      "[user]\n",
      "Using Python and the `pytest` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
      "\n",
      "```python\n",
      "# imports\n",
      "import pytest  # used for our unit tests\n",
      "{insert other imports as needed}\n",
      "\n",
      "# function to test\n",
      "\n",
      "from flask import Flask, session, jsonify, request\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "import os\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "import json\n",
      "\n",
      "\n",
      "\n",
      "#################Load config.json and get path variables\n",
      "with open('config.json','r') as f:\n",
      "    config = json.load(f) \n",
      "\n",
      "output_model_path = os.path.join(config['output_model_path']) \n",
      "test_data_path = os.path.join(config['test_data_path']) \n",
      "\n",
      "\n",
      "#################Function for model scoring\n",
      "def score_model():\n",
      "    #this function should take a trained model, load test data, and calculate an F1 score for the model relative to the test data\n",
      "    #it should write the result to the latestscore.txt file\n",
      "\n",
      "    # Take trained model\n",
      "    model_path = os.path.join(output_model_path, \"trainedmodel.pkl\")\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "\n",
      "    test_df = pd.read_csv(os.path.join(test_data_path, 'testdata.csv'))\n",
      "\n",
      "    y_test = test_df.pop('exited')\n",
      "    X_test = test_df.drop(['corporation'], axis=1)\n",
      "\n",
      "    y_pred = model.predict(X_test)\n",
      "    score = metrics.f1_score(y_test, y_pred)\n",
      "\n",
      "    score_path = os.path.join(output_model_path, \"latestscore.txt\")\n",
      "    with open(score_path, 'w') as f:\n",
      "        f.write(str(score))\n",
      "\n",
      "    return score\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    score = score_model()\n",
      "\n",
      "\n",
      "# unit tests\n",
      "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
      "{insert unit test code here}\n",
      "```\n",
      "\u001b[92m\n",
      "[assistant]\n",
      "```python\n",
      "# imports\n",
      "import pytest\n",
      "from flask import Flask, session, jsonify, request\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "import os\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "import json\n",
      "\n",
      "# function to test\n",
      "def score_model():\n",
      "    #this function should take a trained model, load test data, and calculate an F1 score for the model relative to the test data\n",
      "    #it should write the result to the latestscore.txt file\n",
      "\n",
      "    # Take trained model\n",
      "    model_path = os.path.join(output_model_path, \"trainedmodel.pkl\")\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "\n",
      "    test_df = pd.read_csv(os.path.join(test_data_path, 'testdata.csv'))\n",
      "\n",
      "    y_test = test_df.pop('exited')\n",
      "    X_test = test_df.drop(['corporation'], axis=1)\n",
      "\n",
      "    y_pred = model.predict(X_test)\n",
      "    score = metrics.f1_score(y_test, y_pred)\n",
      "\n",
      "    score_path = os.path.join(output_model_path, \"latestscore.txt\")\n",
      "    with open(score_path, 'w') as f:\n",
      "        f.write(str(score))\n",
      "\n",
      "    return score\n",
      "\n",
      "# unit tests\n",
      "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
      "\n",
      "# Test that the function can load a trained model and test data from the specified paths\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_load_data(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    assert os.path.join(output_model_path) == \"models\"\n",
      "    assert os.path.join(test_data_path) == \"test_data\"\n",
      "\n",
      "# Test that the function can calculate the F1 score for the model relative to the test data\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_score_model(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert score_model() == 1.0\n",
      "\n",
      "# Test that the function can write the F1 score to the latestscore.txt file\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_write_score(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    score_model()\n",
      "    with open(os.path.join(output_model_path, \"latestscore.txt\"), 'r') as f:\n",
      "        assert f.read() == \"1.0\"\n",
      "\n",
      "# Test that the function returns a valid F1 score\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_valid_score(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert isinstance(score_model(), float)\n",
      "    assert score_model() >= 0.0 and score_model() <= 1.0\n",
      "\n",
      "# Test that the function is deterministic\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_deterministic(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert score_model() == score_model()\n",
      "```"
     ]
    }
   ],
   "source": [
    "# example_function = \"\"\"def pig_latin(text):\n",
    "#     def translate(word):\n",
    "#         vowels = 'aeiou'\n",
    "#         if word[0] in vowels:\n",
    "#             return word + 'way'\n",
    "#         else:\n",
    "#             consonants = ''\n",
    "#             for letter in word:\n",
    "#                 if letter not in vowels:\n",
    "#                     consonants += letter\n",
    "#                 else:\n",
    "#                     break\n",
    "#             return word[len(consonants):] + consonants + 'ay'\n",
    "\n",
    "#     words = text.lower().split()\n",
    "#     translated_words = [translate(word) for word in words]\n",
    "#     return ' '.join(translated_words)\n",
    "# \"\"\"\n",
    "\n",
    "example_function =  \"\"\"\n",
    "from flask import Flask, session, jsonify, request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#################Load config.json and get path variables\n",
    "with open('config.json','r') as f:\n",
    "    config = json.load(f) \n",
    "\n",
    "output_model_path = os.path.join(config['output_model_path']) \n",
    "test_data_path = os.path.join(config['test_data_path']) \n",
    "\n",
    "\n",
    "#################Function for model scoring\n",
    "def score_model():\n",
    "    #this function should take a trained model, load test data, and calculate an F1 score for the model relative to the test data\n",
    "    #it should write the result to the latestscore.txt file\n",
    "\n",
    "    # Take trained model\n",
    "    model_path = os.path.join(output_model_path, \"trainedmodel.pkl\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(test_data_path, 'testdata.csv'))\n",
    "\n",
    "    y_test = test_df.pop('exited')\n",
    "    X_test = test_df.drop(['corporation'], axis=1)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    score_path = os.path.join(output_model_path, \"latestscore.txt\")\n",
    "    with open(score_path, 'w') as f:\n",
    "        f.write(str(score))\n",
    "\n",
    "    return score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    score = score_model()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unit_tests = unit_tests_from_function(\n",
    "    example_function,\n",
    "    approx_min_cases_to_cover=10,\n",
    "    print_text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# imports\n",
      "import pytest\n",
      "from flask import Flask, session, jsonify, request\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "import os\n",
      "from sklearn import metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "import json\n",
      "\n",
      "# function to test\n",
      "def score_model():\n",
      "    #this function should take a trained model, load test data, and calculate an F1 score for the model relative to the test data\n",
      "    #it should write the result to the latestscore.txt file\n",
      "\n",
      "    # Take trained model\n",
      "    model_path = os.path.join(output_model_path, \"trainedmodel.pkl\")\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "\n",
      "    test_df = pd.read_csv(os.path.join(test_data_path, 'testdata.csv'))\n",
      "\n",
      "    y_test = test_df.pop('exited')\n",
      "    X_test = test_df.drop(['corporation'], axis=1)\n",
      "\n",
      "    y_pred = model.predict(X_test)\n",
      "    score = metrics.f1_score(y_test, y_pred)\n",
      "\n",
      "    score_path = os.path.join(output_model_path, \"latestscore.txt\")\n",
      "    with open(score_path, 'w') as f:\n",
      "        f.write(str(score))\n",
      "\n",
      "    return score\n",
      "\n",
      "# unit tests\n",
      "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
      "\n",
      "# Test that the function can load a trained model and test data from the specified paths\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_load_data(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    assert os.path.join(output_model_path) == \"models\"\n",
      "    assert os.path.join(test_data_path) == \"test_data\"\n",
      "\n",
      "# Test that the function can calculate the F1 score for the model relative to the test data\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_score_model(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert score_model() == 1.0\n",
      "\n",
      "# Test that the function can write the F1 score to the latestscore.txt file\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_write_score(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    score_model()\n",
      "    with open(os.path.join(output_model_path, \"latestscore.txt\"), 'r') as f:\n",
      "        assert f.read() == \"1.0\"\n",
      "\n",
      "# Test that the function returns a valid F1 score\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_valid_score(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert isinstance(score_model(), float)\n",
      "    assert score_model() >= 0.0 and score_model() <= 1.0\n",
      "\n",
      "# Test that the function is deterministic\n",
      "@pytest.mark.parametrize(\"output_model_path, test_data_path\", [(\"models\", \"test_data\")])\n",
      "def test_deterministic(output_model_path, test_data_path):\n",
      "    config = {'output_model_path': output_model_path, 'test_data_path': test_data_path}\n",
      "    with open('config.json', 'w') as f:\n",
      "        json.dump(config, f)\n",
      "    model = LogisticRegression()\n",
      "    X_train, X_test, y_train, y_test = train_test_split(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([0, 0, 1, 1]), test_size=0.25, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "    with open(os.path.join(output_model_path, \"trainedmodel.pkl\"), 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    test_df = pd.DataFrame({'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'exited': [0, 0, 1, 1], 'corporation': ['A', 'B', 'C', 'D']})\n",
      "    test_df.to_csv(os.path.join(test_data_path, 'testdata.csv'), index=False)\n",
      "    assert score_model() == score_model()\n"
     ]
    }
   ],
   "source": [
    "print(unit_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to check any code before using it, as GPT makes plenty of mistakes (especially on character-based tasks like this one). For best results, use the most powerful model (GPT-4, as of May 2023)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
