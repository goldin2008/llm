{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb8d077-abed-4fbc-9d68-2510e74b0fb0",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is the concept of providing large language models (LLMs) with additional information from an external knowledge source. This allows them to generate more accurate and contextual answers while reducing hallucinations. In this article, we will provide a step-by-step guide to building a complete RAG application using the latest open-source LLM by Google Gemma 7B and open source vector database by Faiss.\n",
    "\n",
    "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed.\n",
    "When a user asks a question to the LLM. Instead of asking the LLM directly, we generate embeddings for this query and then retrieve the relevant data from our knowledge library that is well maintained and then use that context to return the answer.\n",
    "We use vector embeddings (numerical representations) to retrieve the requested document. Once the needed information is found from the vector databases, the result is returned to the user.\n",
    "This largely reduces the possibility of hallucinations and updates the model without retraining the model, which is a costly process. Hereâ€™s a very simple diagram that shows the process.\n",
    "\n",
    "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* LLM - Large Language Model  \n",
    "* Llama 2.0 - LLM from Meta \n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
    "* ChromaDB - vector database  \n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "## Model details\n",
    "\n",
    "* **Model**: Llama 2  \n",
    "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n",
    "\n",
    "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44722f1-5747-4187-b55b-c4ad5ba45e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U langchain torch transformers sentence-transformers datasets faiss-cpu langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8604f-0815-4af3-8f17-e8390b243d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda1f54-7561-4a8c-8aa6-28cc424b07f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18fae84-8b98-43ee-ad48-35d43cf36be5",
   "metadata": {},
   "source": [
    "When you want to deal with long pieces of text, it is necessary to split them into chunks. As simple as this sounds, there is a lot of potential complexity here. Keep the semantically related pieces of text together.\n",
    "\n",
    "LangChain has many built-in document transformers, making it easy to split, combine, filter, and otherwise manipulate documents. We will use the RecursiveCharacterTextSplitter which recursively tries to split by different characters to find one that works with. We will set the chunk size = 1000 and chunk overlap = 150."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a2fa-76e1-4c3d-a5f5-06b91b3ec173",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We used Langchain, FAISS and `Llama 3 as a LLM` to build a Retrieval Augmented Generation solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cf6df-9a23-4c03-85d7-55c5a4ad9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423de25b-53a4-4ee4-8866-bb0dbfec132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "model_id = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0c028-f845-441f-a36f-24fdf20ada75",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2886b-cf3b-46b2-8ea4-640ac89698ff",
   "metadata": {},
   "source": [
    "Gemma is a family of 4 new LLM models by Google based on Gemini. It comes in two sizes: 2B and 7B parameters, each with base (pretrained) and instruction-tuned versions. All the variants can be run on various types of consumer hardware, even without quantization, and have a context length of 8K tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc1422-6c49-4d1b-9640-c95ab20549b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dfe06-9b1a-41b2-8132-f6f121f95951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f735aaf-956b-47b3-891e-766fa7e6baf4",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Create a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d08ad-63d1-4eb1-8f64-564ddb6cfd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0e322-4f7c-48c6-8203-0efde490d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        # return_tensors='pt',\n",
    "        torch_dtype=torch.float16,\n",
    "        max_length=1024,\n",
    "        # device=\"cuda\",\n",
    "        device_map=\"auto\",)\n",
    "time_end = time()\n",
    "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a293183-f077-407a-a5db-563efb425691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, message):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        message: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"    \n",
    "    time_start = time()\n",
    "    sequences = pipeline(\n",
    "        message,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(message)]\n",
    "    answer = sequences[0]['generated_text'][len(message):]\n",
    "    \n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\"\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42147cfb-7a2d-4e4f-b607-01b66181eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"Please explain what is EU AI Act.\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f0c19-e6d8-4f4d-a300-08cc4446db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"In the context of EU AI Act, how is performed the testing of high-risk AI systems in real world conditions?\")\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d91b-1866-4fc7-b289-c53fc205a8ca",
   "metadata": {},
   "source": [
    "The final step is to generate the answers using both the vector store and the LLM. It will generate embeddings to the input query or question retrieve the context from the vector store, and feed this to the LLM to generate the answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e176ed4-84ec-4d14-ac05-47d013bc6947",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68806cd-cf64-45c3-8340-76236fb95f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6d625-5cd8-4d55-9ac6-d562edeb8e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ebf1d-4e01-4253-960f-6b00ce595043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=pipe,\n",
    "#     model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    "# )\n",
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "\n",
    "# checking again that everything is working fine\n",
    "time_start = time()\n",
    "question = \"Please explain what EU AI Act is.\"\n",
    "response = llm(prompt=question)\n",
    "time_end = time()\n",
    "total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "full_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "display(Markdown(colorize_text(full_response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc1f3d-1713-4a10-b485-fa0edd525a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n",
    "#                     encoding=\"utf8\")\n",
    "loader = PyPDFLoader(\"/kaggle/input/eu-ai-act-complete-text/aiact_final_draft.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762d74b-3d75-405f-a7e7-2ccb61652ddc",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7dc167-d135-4d44-b482-aa9628b0c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "# gpt4all_embd = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fffd63-3402-46a0-a629-1d17a7b4b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "# model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2\n",
    "try:\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    # alternatively, we will access the embeddings models locally\n",
    "    local_model_path = \"/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96561fc-07b7-43ea-8c5c-7cd9d6bfa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectordb = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectordb.as_retriever(), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# qa.invoke(\"Write an educational story for young children.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc579f07-ea52-4f56-a4e6-20ad67c59404",
   "metadata": {},
   "source": [
    "#### TEST RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c3a9d-7514-4866-bf96-0c2d2b269a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(qa, query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval Augmented Generation (RAG) system.\n",
    "    \n",
    "    Args:\n",
    "        qa (RetrievalQA.from_chain_type): Langchain function to perform RAG\n",
    "        query (str): query for the RAG system\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    time_start = time()\n",
    "    response = qa.run(query)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "\n",
    "    full_response =  f\"Question: {query}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "    display(Markdown(colorize_text(full_response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebacb1-6ad1-4078-8e60-0387fe370a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is performed the testing of high-risk AI systems in real world conditions?\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55401746-33b5-4844-bd4b-92a015a84e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In what cases a company that develops AI solutions should obtain permission to deploy it?\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390a44e-eeff-4d0f-9846-cb35d3370fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(query)\n",
    "# docs_and_score=vectordb.similarity_search_with_score(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053a800-8ccd-421c-9c17-59bd05a7c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving And Loading\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "new_db=FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "docs=new_db.similarity_search(query)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc1e6d-1416-4c00-bc93-d6559679b692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
