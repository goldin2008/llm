{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45fe9b44-74a5-47be-9199-9266314dd472",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected [[0.73563765 0.36498089 0.76274851 0.67735642]\n [0.73918264 0.36852587 0.76629349 0.68090141]\n [0.74339558 0.37273882 0.77050643 0.68511435]\n [0.73292495 0.36226819 0.76003581 0.67464372]] to have shape (1, 4), but was (4, 4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_49794/3119359451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;31m# train over epochs, calculate MSE at each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     weights, biases = train(\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_49794/3119359451.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, hidden_layer_weights, hidden_layer_biases, output_layer_weights, output_layer_biases)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         new_weights, new_biases = get_new_weights_and_biases(\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mhidden_layer_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_49794/3119359451.py\u001b[0m in \u001b[0;36mget_new_weights_and_biases\u001b[0;34m(training_batch, hidden_layer_weights, hidden_layer_biases, output_layer_weights, output_layer_biases)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mhidden_layer_weighted_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_hidden_layer_weighted_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mhidden_layer_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_weighted_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0moutput_layer_weighted_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_output_layer_weighted_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Compute gradients for this training example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_49794/3119359451.py\u001b[0m in \u001b[0;36mcompute_output_layer_weighted_input\u001b[0;34m(hidden_layer_activation, output_layer_weights, output_layer_biases)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# return the weighted inputs (before applying sigmoid) for output layer as a 1x1 matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0massert_has_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_LAYER_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0massert_has_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_LAYER_WEIGHTS_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0massert_has_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_LAYER_BIASES_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lj/sq_z3m_s3z51_g2s4qfv2hrw0000gn/T/ipykernel_49794/3119359451.py\u001b[0m in \u001b[0;36massert_has_shape\u001b[0;34m(val, expected_shape)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massert_has_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected {val} to have shape {expected_shape}, but was {val.shape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected [[0.73563765 0.36498089 0.76274851 0.67735642]\n [0.73918264 0.36852587 0.76629349 0.68090141]\n [0.74339558 0.37273882 0.77050643 0.68511435]\n [0.73292495 0.36226819 0.76003581 0.67464372]] to have shape (1, 4), but was (4, 4)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "#############################################################\n",
    "####################### INSTRUCTIONS ########################\n",
    "#############################################################\n",
    "\"\"\"\n",
    "Goal: Implement a simple Neural Network to for binary classification over\n",
    "2-dimensional input data. The network must have 1 hidden layer containing 4 units.\n",
    "\n",
    "The network should utilize mini-batch training, a sigmoid activation function, and\n",
    "MSE loss function. The batch size is set in the \n",
    "\"Parameters\" section.\n",
    "\n",
    "Please fill in the 4 functions in the section labeled \"Fill In These Functions.\"\n",
    "The function signatures must not change, and must return appropriate outputs based \n",
    "on the in-line comments within them. You may add additional functions as you see fit.\n",
    "You may leverage the functions in the \"Utilities\" section if you find it necessary.\n",
    "You may change N, LEARNING_RATE, BATCH_SIZE, NUM_EPOCHS if it helps you train your \n",
    "network. Do not modify the code in the section labeled \"Do Not Modify Below.\"\n",
    "Code in this section will call your functions, so make sure your implementation\n",
    "is compatible.\n",
    "\n",
    "Your code must run (you can test it by clicking \"Run\" button in the top-left).\n",
    "The \"train\" method will train your network over NUM_EPOCHS epochs, and print a \n",
    "mean-squared error over the hold-out set after each epoch.\n",
    "\n",
    "Please feel free to add extra print statements if it helps you debug your code.\n",
    "\n",
    "This exercise is open-book. You may leverage resources you find on the Internet, \n",
    "such as syntax references, mathematical formulae, etc., but you should not adapt \n",
    "or otherwise use existing implementation code.\n",
    "\n",
    "The following PDF reference may be useful to you:\n",
    "https://drive.google.com/file/d/1zbRSQJaPrQHYOGzPYLm6mbFLUHhVxlm2/view?usp=sharing\n",
    "\"\"\"\n",
    "\n",
    "#############################################################\n",
    "######################### PARAMETERS ########################\n",
    "#############################################################\n",
    "N = 1000\n",
    "LEARNING_RATE = 1\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 10\n",
    "INPUT_WIDTH = 2\n",
    "HIDDEN_LAYER_WIDTH = 4\n",
    "OUTPUT_LAYER_WIDTH = 1\n",
    "HIDDEN_LAYER_WEIGHTS_SHAPE = (HIDDEN_LAYER_WIDTH, INPUT_WIDTH)\n",
    "HIDDEN_LAYER_BIASES_SHAPE = (HIDDEN_LAYER_WIDTH, 1)\n",
    "OUTPUT_LAYER_WEIGHTS_SHAPE = (OUTPUT_LAYER_WIDTH, HIDDEN_LAYER_WIDTH)\n",
    "OUTPUT_LAYER_BIASES_SHAPE = (OUTPUT_LAYER_WIDTH, 1)\n",
    "INITIAL_HIDDEN_LAYER_WEIGHTS = np.random.random(HIDDEN_LAYER_WEIGHTS_SHAPE)\n",
    "INITIAL_HIDDEN_LAYER_BIASES = np.random.random(HIDDEN_LAYER_BIASES_SHAPE)\n",
    "INITIAL_OUTPUT_LAYER_WEIGHTS = np.random.random(OUTPUT_LAYER_WEIGHTS_SHAPE)\n",
    "INITIAL_OUTPUT_LAYER_BIASES = np.random.random(OUTPUT_LAYER_BIASES_SHAPE)\n",
    "\n",
    "#############################################################\n",
    "######################### UTILITIES #########################\n",
    "#############################################################\n",
    "def sigmoid(z):\n",
    "    # activation function\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # derivative of activation function\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def assert_has_shape(val, expected_shape):\n",
    "    assert val.shape == expected_shape, f\"Expected {val} to have shape {expected_shape}, but was {val.shape}.\"\n",
    "\n",
    "\n",
    "#############################################################\n",
    "################## FILL IN THESE FUNCTIONS ##################\n",
    "#############################################################\n",
    "def compute_hidden_layer_weighted_input(x, hidden_layer_weights, hidden_layer_biases):\n",
    "    # return the weighted inputs (before applying sigmoid) for layer 1 as a 4x1 matrix\n",
    "    assert_has_shape(x, (2, 1))\n",
    "    assert_has_shape(hidden_layer_weights, HIDDEN_LAYER_WEIGHTS_SHAPE)\n",
    "    assert_has_shape(hidden_layer_biases, HIDDEN_LAYER_BIASES_SHAPE)\n",
    "\n",
    "    # fill in\n",
    "    # return np.random.random((HIDDEN_LAYER_WIDTH, 1))\n",
    "    # Compute the weighted input using the dot product of weights and input and add biases.\n",
    "    weighted_input = np.dot(hidden_layer_weights, x) + hidden_layer_biases\n",
    "    return weighted_input\n",
    "\n",
    "\n",
    "def compute_output_layer_weighted_input(\n",
    "    hidden_layer_activation, output_layer_weights, output_layer_biases\n",
    "):\n",
    "    # return the weighted inputs (before applying sigmoid) for output layer as a 1x1 matrix\n",
    "    assert_has_shape(hidden_layer_activation, (HIDDEN_LAYER_WIDTH, 1))\n",
    "    assert_has_shape(output_layer_weights, OUTPUT_LAYER_WEIGHTS_SHAPE)\n",
    "    assert_has_shape(output_layer_biases, OUTPUT_LAYER_BIASES_SHAPE)\n",
    "\n",
    "    # fill in\n",
    "    # return np.random.random((OUTPUT_LAYER_WIDTH, 1))\n",
    "    # Compute the weighted input using the dot product of weights and input and add biases.\n",
    "    weighted_input = np.dot(output_layer_weights, hidden_layer_activation) + output_layer_biases\n",
    "    return weighted_input\n",
    "\n",
    "\n",
    "def compute_gradients(\n",
    "    x,\n",
    "    y,\n",
    "    hidden_layer_weights,\n",
    "    hidden_layer_biases,\n",
    "    hidden_layer_weighted_input,\n",
    "    output_layer_weights,\n",
    "    output_layer_biases,\n",
    "    output_layer_weighted_input,\n",
    "):\n",
    "    # x, y is a single training example\n",
    "    # for a single training example, return the gradient of loss with respect to each layer's weights and biases\n",
    "    # return value should be a tuple of lists, where the first element is the list of weight gradients,\n",
    "    # and the second is the list of bias gradients. the shape of each \"gradient\" should correspond to the shape of the\n",
    "    # weight/bias matrix it will be used to update.\n",
    "    assert_has_shape(x, (2, 1))\n",
    "    assert_has_shape(hidden_layer_weights, HIDDEN_LAYER_WEIGHTS_SHAPE)\n",
    "    assert_has_shape(hidden_layer_biases, HIDDEN_LAYER_BIASES_SHAPE)\n",
    "    assert_has_shape(output_layer_weights, OUTPUT_LAYER_WEIGHTS_SHAPE)\n",
    "    assert_has_shape(output_layer_biases, OUTPUT_LAYER_BIASES_SHAPE)\n",
    "\n",
    "    # fill in\n",
    "    # weight_gradients = [\n",
    "    #     np.zeros((HIDDEN_LAYER_WIDTH, INPUT_WIDTH)),\n",
    "    #     np.zeros(OUTPUT_LAYER_WIDTH, HIDDEN_LAYER_WIDTH),\n",
    "    # ]\n",
    "    # bias_gradients = [\n",
    "    #     np.zeros((HIDDEN_LAYER_WIDTH, 1)),\n",
    "    #     np.zeros((OUTPUT_LAYER_WIDTH, 1)),\n",
    "    # ]\n",
    "    # return weight_gradients, bias_gradients\n",
    "    \n",
    "    # Calculate gradients for weights and biases of both layers.\n",
    "\n",
    "    # Calculate the error in the output layer.\n",
    "    output_layer_error = sigmoid(output_layer_weighted_input) - y\n",
    "    \n",
    "    # Calculate the gradient for the output layer weights and biases.\n",
    "    output_layer_weight_gradients = output_layer_error * sigmoid_prime(output_layer_weighted_input) * sigmoid(hidden_layer_weighted_input)\n",
    "    output_layer_bias_gradients = output_layer_error * sigmoid_prime(output_layer_weighted_input) * 1\n",
    "    \n",
    "    # # Calculate the error in the hidden layer.\n",
    "    hidden_layer_error = np.dot(output_layer_weights.T, output_layer_error)\n",
    "    \n",
    "    # Calculate the gradient for the hidden layer weights and biases.\n",
    "    # hidden_layer_weight_gradients = output_layer_error * sigmoid_prime(output_layer_weighted_input) * output_layer_weights * sigmoid_prime(hidden_layer_weighted_input) * x.T\n",
    "    # hidden_layer_bias_gradients = output_layer_error * sigmoid_prime(output_layer_weighted_input) * output_layer_weights * sigmoid_prime(hidden_layer_weighted_input) * 1\n",
    "    hidden_layer_weight_gradients = hidden_layer_error * sigmoid_prime(hidden_layer_weighted_input) * x.T\n",
    "    hidden_layer_bias_gradients = hidden_layer_error * sigmoid_prime(hidden_layer_weighted_input)\n",
    "    \n",
    "    # Return the gradients.\n",
    "    weight_gradients = [hidden_layer_weight_gradients, output_layer_weight_gradients]\n",
    "    bias_gradients = [hidden_layer_bias_gradients, output_layer_bias_gradients]\n",
    "    \n",
    "    return weight_gradients, bias_gradients\n",
    "\n",
    "\n",
    "def get_new_weights_and_biases(\n",
    "    training_batch,\n",
    "    hidden_layer_weights,\n",
    "    hidden_layer_biases,\n",
    "    output_layer_weights,\n",
    "    output_layer_biases,\n",
    "):\n",
    "    # training_batch is a list of (x, y) training examples\n",
    "    # return the new weights and biases after processing this batch of data, and according to LEARNING_RATE\n",
    "\n",
    "    # fill in\n",
    "    # new_weights = [hidden_layer_weights, output_layer_weights]\n",
    "    # new_biases = [hidden_layer_biases, output_layer_biases]\n",
    "    # return new_weights, new_biases\n",
    "\n",
    "    # Update the weights and biases based on the training batch and learning rate.\n",
    "\n",
    "    # Initialize the accumulators for gradients.\n",
    "    total_weight_gradients = [np.zeros_like(w) for w in [hidden_layer_weights, output_layer_weights]]\n",
    "    total_bias_gradients = [np.zeros_like(b) for b in [hidden_layer_biases, output_layer_biases]]\n",
    "    \n",
    "    for x, y in training_batch:\n",
    "        # Compute the weighted inputs for both layers.\n",
    "        hidden_layer_weighted_input = compute_hidden_layer_weighted_input(x, hidden_layer_weights, hidden_layer_biases)\n",
    "        hidden_layer_activation = sigmoid(hidden_layer_weighted_input)\n",
    "        output_layer_weighted_input = compute_output_layer_weighted_input(hidden_layer_activation, output_layer_weights, output_layer_biases)\n",
    "        \n",
    "        # Compute gradients for this training example.\n",
    "        weight_gradients, bias_gradients = compute_gradients(\n",
    "            x, y, hidden_layer_weights, hidden_layer_biases, hidden_layer_weighted_input,\n",
    "            output_layer_weights, output_layer_biases, output_layer_weighted_input\n",
    "        )\n",
    "        \n",
    "        # Accumulate gradients.\n",
    "        total_weight_gradients = [twg + wg for twg, wg in zip(total_weight_gradients, weight_gradients)]\n",
    "        total_bias_gradients = [tbg + bg for tbg, bg in zip(total_bias_gradients, bias_gradients)]\n",
    "    \n",
    "    # Update weights and biases using the accumulated gradients and learning rate.\n",
    "    learning_rate = LEARNING_RATE\n",
    "    new_weights = [w - learning_rate * g for w, g in zip([hidden_layer_weights, output_layer_weights], total_weight_gradients)]\n",
    "    new_biases = [b - learning_rate * g for b, g in zip([hidden_layer_biases, output_layer_biases], total_bias_gradients)]\n",
    "    \n",
    "    return new_weights, new_biases\n",
    "\n",
    "    \n",
    "#############################################################\n",
    "#################### DO NOT MODIFY BELOW ####################\n",
    "#############################################################\n",
    "def predict(\n",
    "    x,\n",
    "    hidden_layer_weights,\n",
    "    hidden_layer_biases,\n",
    "    output_layer_weights,\n",
    "    output_layer_biases,\n",
    "):\n",
    "    hidden_layer_activation = sigmoid(compute_hidden_layer_weighted_input(x, hidden_layer_weights, hidden_layer_biases))\n",
    "    output_layer_activation = sigmoid(compute_output_layer_weighted_input(hidden_layer_activation, output_layer_weights, output_layer_biases))\n",
    "    return output_layer_activation[0][0]\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    Y,\n",
    "    hidden_layer_weights,\n",
    "    hidden_layer_biases,\n",
    "    output_layer_weights,\n",
    "    output_layer_biases,\n",
    "):\n",
    "    # X is an array of (2 x 1) input instances\n",
    "    # Y is an array of scalar targets\n",
    "    for batch_start in range(0, len(X), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        x_batch = X[batch_start:batch_end]\n",
    "        y_batch = Y[batch_start:batch_end]\n",
    "        batch = list(zip(x_batch, y_batch))\n",
    "\n",
    "        new_weights, new_biases = get_new_weights_and_biases(\n",
    "            batch,\n",
    "            hidden_layer_weights,\n",
    "            hidden_layer_biases,\n",
    "            output_layer_weights,\n",
    "            output_layer_biases,\n",
    "        )\n",
    "        hidden_layer_weights, output_layer_weights = new_weights\n",
    "        hidden_layer_biases, output_layer_biases = new_biases\n",
    "\n",
    "    # return the final weights and biases\n",
    "    return (\n",
    "        [hidden_layer_weights, output_layer_weights],\n",
    "        [hidden_layer_biases, output_layer_biases],\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_mse(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    hidden_layer_weights,\n",
    "    hidden_layer_biases,\n",
    "    output_layer_weights,\n",
    "    output_layer_biases,\n",
    "):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        predictions.append(\n",
    "            predict(\n",
    "                x,\n",
    "                hidden_layer_weights,\n",
    "                hidden_layer_biases,\n",
    "                output_layer_weights,\n",
    "                output_layer_biases,\n",
    "            )\n",
    "        )\n",
    "    y_hat = np.array(predictions)\n",
    "    return np.mean((y_hat - Y_test) ** 2)\n",
    "\n",
    "\n",
    "# prepare input data\n",
    "X = np.random.choice([0, 1], (N, 2))\n",
    "Y = np.logical_xor(X[:, 0], X[:, 1]) * 1\n",
    "X = X + 0.1 * np.random.random((N, 2))\n",
    "X = [np.array([x]).T for x in X]\n",
    "\n",
    "# split into train and test\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "Y_train, Y_test = Y[:split], Y[split:]\n",
    "\n",
    "# initialize weigths, biases\n",
    "hidden_layer_weights, hidden_layer_biases = (\n",
    "    INITIAL_HIDDEN_LAYER_WEIGHTS,\n",
    "    INITIAL_HIDDEN_LAYER_BIASES,\n",
    ")\n",
    "output_layer_weights, output_layer_biases = (\n",
    "    INITIAL_OUTPUT_LAYER_WEIGHTS,\n",
    "    INITIAL_OUTPUT_LAYER_BIASES,\n",
    ")\n",
    "\n",
    "# train over epochs, calculate MSE at each epoch\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    weights, biases = train(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        hidden_layer_weights,\n",
    "        hidden_layer_biases,\n",
    "        output_layer_weights,\n",
    "        output_layer_biases,\n",
    "    )\n",
    "    hidden_layer_weights, output_layer_weights = weights\n",
    "    hidden_layer_biases, output_layer_biases = biases\n",
    "    epoch_mse = compute_mse(\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        hidden_layer_weights,\n",
    "        hidden_layer_biases,\n",
    "        output_layer_weights,\n",
    "        output_layer_biases,\n",
    "    )\n",
    "    print(f\"MSE (epoch {epoch}):\", epoch_mse)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afe787-3470-48dd-8206-de982c3a4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
