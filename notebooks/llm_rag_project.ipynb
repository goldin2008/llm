{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb8d077-abed-4fbc-9d68-2510e74b0fb0",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is the concept of providing large language models (LLMs) with additional information from an external knowledge source. This allows them to generate more accurate and contextual answers while reducing hallucinations. In this article, we will provide a step-by-step guide to building a complete RAG application using the latest open-source LLM by Google Gemma 7B and open source vector database by Faiss.\n",
    "\n",
    "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed.\n",
    "When a user asks a question to the LLM. Instead of asking the LLM directly, we generate embeddings for this query and then retrieve the relevant data from our knowledge library that is well maintained and then use that context to return the answer.\n",
    "We use vector embeddings (numerical representations) to retrieve the requested document. Once the needed information is found from the vector databases, the result is returned to the user.\n",
    "This largely reduces the possibility of hallucinations and updates the model without retraining the model, which is a costly process. Hereâ€™s a very simple diagram that shows the process.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* LLM - Large Language Model  \n",
    "* Llama 2.0 - LLM from Meta \n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
    "* ChromaDB - vector database  \n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "## Model details\n",
    "\n",
    "* **Model**: Llama 2  \n",
    "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n",
    "\n",
    "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44722f1-5747-4187-b55b-c4ad5ba45e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U langchain torch transformers sentence-transformers datasets faiss-cpu langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8604f-0815-4af3-8f17-e8390b243d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda1f54-7561-4a8c-8aa6-28cc424b07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"HuggingFaceTB/cosmopedia\", \"stanford\", split=\"train\")\n",
    "# data.to_csv(\"stanford_dataset.csv\")\n",
    "# data.head()\n",
    "# loader = CSVLoader(file_path='/kaggle/working/stanford_dataset.csv')\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fae84-8b98-43ee-ad48-35d43cf36be5",
   "metadata": {},
   "source": [
    "When you want to deal with long pieces of text, it is necessary to split them into chunks. As simple as this sounds, there is a lot of potential complexity here. Keep the semantically related pieces of text together.\n",
    "\n",
    "LangChain has many built-in document transformers, making it easy to split, combine, filter, and otherwise manipulate documents. We will use the RecursiveCharacterTextSplitter which recursively tries to split by different characters to find one that works with. We will set the chunk size = 1000 and chunk overlap = 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cf6df-9a23-4c03-85d7-55c5a4ad9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "# docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423de25b-53a4-4ee4-8866-bb0dbfec132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "# model_kwargs = {'device':'cpu'}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#  model_name=modelPath, \n",
    "#  model_kwargs=model_kwargs, \n",
    "#  encode_kwargs=encode_kwargs \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0c028-f845-441f-a36f-24fdf20ada75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2886b-cf3b-46b2-8ea4-640ac89698ff",
   "metadata": {},
   "source": [
    "Gemma is a family of 4 new LLM models by Google based on Gemini. It comes in two sizes: 2B and 7B parameters, each with base (pretrained) and instruction-tuned versions. All the variants can be run on various types of consumer hardware, even without quantization, and have a context length of 8K tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc1422-6c49-4d1b-9640-c95ab20549b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dfe06-9b1a-41b2-8132-f6f121f95951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f735aaf-956b-47b3-891e-766fa7e6baf4",
   "metadata": {},
   "source": [
    "Create a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c16c5-04bf-4b2c-b3f9-6b0c08d6cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer,\n",
    "#     return_tensors='pt',\n",
    "#     max_length=512,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device=\"cuda\"\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=pipe,\n",
    "#     model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d91b-1866-4fc7-b289-c53fc205a8ca",
   "metadata": {},
   "source": [
    "The final step is to generate the answers using both the vector store and the LLM. It will generate embeddings to the input query or question retrieve the context from the vector store, and feed this to the LLM to generate the answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ebf1d-4e01-4253-960f-6b00ce595043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=db.as_retriever()\n",
    "# )\n",
    "\n",
    "# qa.invoke(\"Write an educational story for young children.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
