{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358be6c3-6244-4c80-80ea-ef7acfbf132a",
   "metadata": {},
   "source": [
    "Now, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon.\n",
    "\n",
    "PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).\n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request. The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3523ba-13a2-4607-995b-5a41906f23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q bitsandbytes\n",
    "%pip install -q transformers\n",
    "%pip install -q peft\n",
    "%pip install -q accelerate\n",
    "%pip install -q trl\n",
    "%pip install -q torch\n",
    "%pip install -q qdrant-client langchain pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13509d4-735c-4459-a831-aa7d0c2fd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, GenerationConfig, Trainer\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28132f-5afe-4eaa-9220-e435f4b50f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length, \n",
    "                  input_max_text_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Tokenizer model name.\n",
    "    - dataset_name (str): Name of the dataset to load.\n",
    "    - input_min_text_length (int): Minimum length of the dialogues.\n",
    "    - input_max_text_length (int): Maximum length of the dialogues.\n",
    "        \n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load dataset (only \"train\" part will be enough for this lab).\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200, \n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bfd89-1fd6-4922-8134-cc24abf3999c",
   "metadata": {},
   "source": [
    "### Finetuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19007e6-6a8f-47a3-8dcb-bdd5a1c81b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"/kaggle/input/gemma/transformers/2b-it/2\"\n",
    "\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, quantization_config=bnbConfig, device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map = \"auto\",\n",
    "    quantization_config=bnbConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cc8c6-c3af-469a-82fd-cb8e48491d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n",
    "user = \"Write a Python code to display text in a star pattern.\"\n",
    "\n",
    "prompt = f\"System: {system} \\n User: {user} \\n AI: \"\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=1000)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "Markdown(text.split(\"AI:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adb2a7-b899-4009-8aa5-cb32b8a952d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\")\n",
    "dataset = Dataset.from_pandas(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43569-e04c-4ecb-8d7d-2aa6c9965bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "    line = template.format(instruction=example['Question'], response=example['Answer'])\n",
    "    return [line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dd1cb-2922-44ec-ab8a-9de101432b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53e092-ca79-4124-a9b8-d0601b24efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c34695-f247-4649-aad8-df7fc5d63555",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb719d-6e09-460d-9c24-c5bc359a2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "system =  \"You are a skilled software engineer who consistently produces high-quality Python code.\"\n",
    "question =system + \"What is the difference between a variable and an object\"\n",
    "\n",
    "prompt = f\"Question: {question} \\n Answer: \"\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=512)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "Markdown(text.split(\"Answer:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bea22-8d87-45ef-abbe-ae950ceaba08",
   "metadata": {},
   "source": [
    "### Finetuning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880da29-ceb7-4919-874f-8f9f43463d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, # Rank\n",
    "    lora_alpha=4,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133abc7-31c2-42f4-b8da-e64580d15139",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a7c9c-c38a-4e42-adb7-c1bf012d37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c9b4c-f816-4003-9679-8405d3d27ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15d43c-c141-4a2e-8cf6-e422017fcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7646d5-8346-45a6-9420-1ccb5cde031a",
   "metadata": {},
   "source": [
    "### Finetuning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11c1e0-577a-4195-b5b7-a6baf7d2efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5adc92-4e4f-4216-9672-1fce8bf4517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc71dc9-0d24-47b0-a62c-93281dd20680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4ff67-2af3-435e-b6af-91e8a631d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('/kaggle/input/databricks-dolly-15k/databricks-dolly-15k.jsonl') as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        # Filter out examples with context, to keep it simple.\n",
    "        if features[\"context\"]:\n",
    "            continue\n",
    "        # Format the entire example as a single string.\n",
    "        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# Only use 1000 training examples, to keep it fast.\n",
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddafccd-2370-43f3-9e63-705fe0ed1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d743e-1ddb-4804-90af-1bb19a3bd531",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b12bd7-34fd-48a4-b424-b7da6a08f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "original_model = gemma_lm\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80821388-b5a7-4330-a1bb-3ab8a8930820",
   "metadata": {},
   "source": [
    "Inference before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddee584-d570-4771-82da-eec62de0de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289fd65-2717-4b51-be5c-f65dded130af",
   "metadata": {},
   "source": [
    "LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbb96c-6759-4d49-95ee-9d954e608d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62d475-5d1c-43a5-84a3-0be2699f28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3aa6d0-70d2-4d5d-a48f-b4b8552914d9",
   "metadata": {},
   "source": [
    "Inference after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc83ea2-4399-40bd-ba68-d4b4871b623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750a6a8-5ef1-4294-9312-54dd46bbc7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e73da4-dbe6-4809-bb70-f2bc1fee8249",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ad1b4-7b0e-456a-add5-0b3cd262a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PyPDFDirectoryLoader object with the specified directory path\n",
    "pdf_loader = PyPDFDirectoryLoader(\"/kaggle/input/knowledge-base\")\n",
    "\n",
    "# Load PDF documents from the specified directory\n",
    "pdfs = pdf_loader.load()\n",
    "\n",
    "# Instantiate a RecursiveCharacterTextSplitter object with specified parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split documents into chunks using the RecursiveCharacterTextSplitter\n",
    "all_splits = text_splitter.split_documents(pdfs)\n",
    "\n",
    "# import the HuggingFaceEmbeddings class, \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # This argument specifies the pre-trained model name to be used for generating embeddings.\n",
    "    # Here, \"sentence-transformers/all-mpnet-base-v2\" is a pre-trained sentence transformer model \n",
    "    # from the Sentence Transformers library (not Transformers).\n",
    "    # Sentence transformer models are specifically trained to generate meaningful representations \n",
    "    # of sentences that capture semantic similarity.\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\n",
    "    # This argument is likely specific to the HuggingFaceEmbeddings class and might \n",
    "    # not be present in the base Transformers library.\n",
    "    # It sets the device to \"cuda\" to leverage the GPU for faster processing if available.\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8d093-967a-4491-972b-24ee4e7902f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant collection from the document splits\n",
    "# For storing and searching document information we use a vector database called Qdrant. \n",
    "\n",
    "qdrant_collection = Qdrant.from_documents(\n",
    "    all_splits,                # List of document splits\n",
    "    embeddings,                # HuggingFaceEmbeddings object for generating embeddings\n",
    "    location=\":memory:\",       # Location to store the collection (in memory)\n",
    "    collection_name=\"all_documents\"  # Name of the Qdrant collection\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = qdrant_collection.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f31b7-d8d6-42cf-ae3a-6bd1d193ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a pipeline for text generation using a pre-trained model (model) \n",
    "# and its tokenizer (tokenizer). It leverages mixed precision (torch.bfloat16) \n",
    "# for potentially faster inference and limits generated text to 512 tokens.\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs = {\"torch.dtype\": torch.bfloat16},\n",
    "    max_new_tokens=512    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff766482-18b0-4af6-84cc-1e64a929e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the difference between a variable and an object\"\n",
    "\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    add_special_tokens=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=10,\n",
    "    top_p=0.95\n",
    ")\n",
    "Markdown(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b037280-83fa-454a-98dc-1577fc222220",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"add_special_tokens\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    ")\n",
    "# Create a RetrievalQA object\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=gemma_llm,  # Pass the text-generation pipeline object\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever  # retriever object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725a535-05cd-4317-b82b-2903d5b5b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write in detail about python\"\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True, truncation=True)\n",
    "result = qa.invoke(prompt)\n",
    "Markdown(result['result'].split('Helpful Answer:')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
