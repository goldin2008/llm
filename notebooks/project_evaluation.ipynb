{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56259a1a-1b41-4574-be69-47e5a6667c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57cd4d-03fe-4217-bd33-7acd94ad87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model_id, dataset_name):\n",
    "    dataset = load_dataset(dataset_name, split=\"test\")\n",
    "    def format(sample):\n",
    "        return \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\\n\".format(sample[\"instruction\"])\n",
    "    dataset = dataset.map(lambda sample: {\"prompt\": format(sample)})\n",
    "    llm = LLM(model=model_id, max_model_len=4096)\n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, min_p=0.05, max_tokens=4096)\n",
    "    outputs = llm.generate(dataset[\"prompt\"], sampling_params)\n",
    "    answers = [output.outputs[0].text for output in outputs]\n",
    "    dataset = dataset.add_column(\"answers\", answers)\n",
    "    print(f\"Uploading results for {model_id}\")\n",
    "    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')[-1]}-results\")\n",
    "    gc.collect()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78d771-53f2-4437-b8f0-a928a5a71677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    'mlabonne/TwinLlama-3.1-8B',\n",
    "    'mlabonne/TwinLlama-3.1-8B-DPO',\n",
    "    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "]\n",
    "for model_id in model_ids:\n",
    "    generate_answers(model_id, \"mlabonne/llmtwin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2329044-6316-4a9b-a5c3-a216bc602e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from datasets import Dataset, load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb659bf2-0368-4ba6-9150-cf3fef3c36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(\n",
    "    instruction: str, answer: str, client: OpenAI\n",
    ") -> dict:\n",
    "    prompt = f\"\"\"You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria:\n",
    "1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\n",
    "2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or academic language.\n",
    "Accuracy scale:\n",
    "1 (Poor): Contains factual errors or misleading information\n",
    "2 (Good): Mostly accurate with minor errors or omissions\n",
    "3 (Excellent): Highly accurate and comprehensive\n",
    "Style scale:\n",
    "1 (Poor): Too formal, uses some overly complex words\n",
    "2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions\n",
    "3 (Excellent): Perfectly accessible language for blog/social media, uses simple but precise technical terms when necessary\n",
    "Example of bad style: The Llama2 7B model constitutes a noteworthy progression in the field of artificial intelligence, serving as the successor to its predecessor, the original Llama architecture.\n",
    "Example of excellent style: Llama2 7B outperforms the original Llama model across multiple benchmarks.\n",
    "Instruction: {instruction}\n",
    "Answer: {answer}\n",
    "Provide your evaluation in JSON format with the following structure:\n",
    "{{\n",
    "    \"accuracy\": {{\n",
    "        \"analysis\": \"...\",\n",
    "        \"score\": 0\n",
    "    }},\n",
    "    \"style\": {{\n",
    "        \"analysis\": \"...\",\n",
    "        \"score\": 0\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        max_tokens=1000,\n",
    "        temperature=0.8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041acfe4-6546-4ce0-8c83-c4dc7e8616af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(batch, start_index):\n",
    "    client = OpenAI(api_key=OPENAI_KEY)\n",
    "    return [\n",
    "        (i, evaluate_answer(instr, ans, client))\n",
    "        for i, (instr, ans) in enumerate(batch, start=start_index)\n",
    "    ]\n",
    "\n",
    "def evaluate_answers(model_id: str, num_threads: int = 10, batch_size: int = 5) -> Dataset:\n",
    "    dataset = load_dataset(f\"mlabonne/{model_id.split('/')[-1]}-results\", split=\"all\")\n",
    "    batches = [\n",
    "        (i, list(zip(dataset[\"instruction\"][i:i+batch_size], dataset[\"answers\"][i:i+batch_size])))\n",
    "        for i in range(0, len(dataset), batch_size)\n",
    "    ]\n",
    "    evaluations = [None] * len(dataset)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(evaluate_batch, batch, start_index) for start_index, batch in batches]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            for index, evaluation in future.result():\n",
    "                evaluations[index] = evaluation\n",
    "    if 'evaluation' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['evaluation'])\n",
    "    dataset = dataset.add_column(\"evaluation\", evaluations)\n",
    "    accuracy_scores = []\n",
    "    style_scores = []\n",
    "    for evaluation in dataset['evaluation']:\n",
    "        try:\n",
    "            eval_dict = json.loads(evaluation) if isinstance(evaluation, str) else evaluation\n",
    "            accuracy_score = eval_dict['accuracy']['score']\n",
    "            style_score = eval_dict['style']['score']\n",
    "            accuracy_scores.append(accuracy_score)\n",
    "            style_scores.append(style_score)\n",
    "        except (json.JSONDecodeError, KeyError, TypeError):\n",
    "            accuracy_scores.append(None)\n",
    "            style_scores.append(None)\n",
    "    if 'accuracy' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['accuracy'])\n",
    "    dataset = dataset.add_column('accuracy', accuracy_scores)\n",
    "    if 'style' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['style'])\n",
    "    dataset = dataset.add_column('style', style_scores)\n",
    "    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')[-1]}-results\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c102a-63f7-43c8-a34a-657d010723e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    'mlabonne/TwinLlama-3.1-8B',\n",
    "    'mlabonne/TwinLlama-3.1-8B-DPO',\n",
    "    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "]\n",
    "for model_id in model_ids:\n",
    "    evaluate_answers(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
