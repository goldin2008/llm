{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5d3995-405d-4f26-bf27-1068ae0f09fe",
   "metadata": {},
   "source": [
    "1. Install Required Packages\n",
    "\n",
    "First, install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec881c5d-47a2-4595-a940-d82e6279873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers transformers faiss-cpu langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd16556-952c-4cbd-83d2-7ecc3b436912",
   "metadata": {},
   "source": [
    "2. Set Up Embeddings with sentence-transformers\n",
    "\n",
    "We'll use a pre-trained model from sentence-transformers to generate embeddings for our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7eed9c-b0ae-4e75-8128-083a9f98a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiyu/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/leiyu/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained sentence transformer model locally\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The revenue for the last quarter was $10 million.\",\n",
    "    \"Our operating income increased by 15% compared to the previous year.\",\n",
    "    \"The company plans to launch a new product next quarter.\",\n",
    "    \"Our net profit margin has improved by 5%.\",\n",
    "    \"We are investing heavily in research and development.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embedder.encode(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493abf13-9264-4af4-a862-ea9d294f6c22",
   "metadata": {},
   "source": [
    "3. Use FAISS for Vector Store\n",
    "\n",
    "We'll use FAISS, an efficient similarity search library, to store and retrieve the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4477282-1496-4456-bc1f-b0fe66ee62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = document_embeddings.shape[1]  # Dimensions of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance is a common choice\n",
    "\n",
    "# Add document embeddings to the index\n",
    "index.add(np.array(document_embeddings))\n",
    "\n",
    "# Optional: Create a function to map index to document\n",
    "def search_faiss(query_embedding, k=5):\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return indices[0], distances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4968e-389f-49c5-9083-a6643027de1b",
   "metadata": {},
   "source": [
    "4. Set Up a Local Language Model with Hugging Face Transformers\n",
    "\n",
    "Now, let's set up a local language model that will generate answers based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03274ab-15fc-4dfe-a75e-2080b6356cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiyu/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a local language model for text generation\n",
    "# (you may use 'gpt2' or any other model available locally)\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Define a function to generate answers\n",
    "def generate_answer(context, query):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e717711-58c1-492c-bf0d-a27d98605176",
   "metadata": {},
   "source": [
    "5. Integrate Retrieval and Generation\n",
    "\n",
    "Let's combine everything into a function that takes a query, retrieves the most relevant documents, and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ebeeb5-5967-4482-b8ab-a4f70a5b501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, k=3):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.encode(query)\n",
    "\n",
    "    # Retrieve the top-k relevant documents\n",
    "    indices, distances = search_faiss(query_embedding, k=k)\n",
    "    relevant_docs = [documents[i] for i in indices]\n",
    "\n",
    "    # Combine the relevant documents into a single context\n",
    "    context = \" \".join(relevant_docs)\n",
    "\n",
    "    # Generate the answer using the local language model\n",
    "    answer = generate_answer(context, query)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c029dfb-99d3-4863-9a1b-bae52c6edfb6",
   "metadata": {},
   "source": [
    "6. Run the Q&A System\n",
    "\n",
    "Finally, you can run the system with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31202e98-6889-41ce-986d-10577b40151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The revenue for the last quarter was $10 million. Our operating income increased by 15% compared to the previous year. The company plans to launch a new product next quarter.\n",
      "\n",
      "Question: What is the revenue for the last quarter?\n",
      "Answer: Our total revenue was $0.33 billion in the quarter ended June 30, 2015 compared to what was reported for that quarter from Q3 2016, or $2.15 bx.\n",
      "\n",
      "Question: The next quarter sales are\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the revenue for the last quarter?\"\n",
    "answer = answer_question(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fab64-5ff6-46aa-845c-4e64ccc49d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
